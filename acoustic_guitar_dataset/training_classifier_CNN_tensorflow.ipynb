{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREPROCESSED_X_PATH = './data/preprocessed_X.pkl'\n",
    "PREPROCESSED_Y_PATH = './data/preprocessed_y.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape:  (3214, 12, 80, 1)\n",
      "Reshaped:  (3214, 960)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = np.load(PREPROCESSED_X_PATH)\n",
    "y = np.load(PREPROCESSED_Y_PATH)\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "#Split into train, test and validation data\n",
    "X_train = np.array(X[:int(len(X)*0.7)], dtype=np.float32)\n",
    "y_train = np.array(y[:int(len(y)*0.7)], dtype=np.float32)\n",
    "\n",
    "X_test = np.array(X[int(len(X)*0.9):], dtype=np.float32)\n",
    "y_test = np.array(y[int(len(y)*0.9):], dtype=np.float32)\n",
    "\n",
    "X_valid = np.array(X[int(len(X)*0.7):int(len(X)*0.9)], dtype=np.float32)\n",
    "y_valid = np.array(y[int(len(y)*0.7):int(len(y)*0.9)], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "#building a CNN using tensorflow\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "class CNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None, \n",
    "                 height=12, width=80, channels=1, architecture=1, \n",
    "                 conv1={'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':None, 'conv1_activation':tf.nn.elu},\n",
    "                 conv2={'conv2_fmaps':32, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.2, 'conv2_activation':tf.nn.elu}):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.conv1_fmaps = conv1['conv1_fmaps']\n",
    "        self.conv1_ksize = conv1['conv1_ksize']\n",
    "        self.conv1_stride = conv1['conv1_stride']\n",
    "        self.conv1_dropout = conv1['conv1_dropout']\n",
    "        self.conv1_activation = conv1['conv1_activation']\n",
    "        self.conv2_fmaps = conv2['conv2_fmaps']\n",
    "        self.conv2_ksize = conv2['conv2_ksize']\n",
    "        self.conv2_stride = conv2['conv2_stride']\n",
    "        self.conv2_dropout = conv2['conv2_dropout']\n",
    "        self.conv2_activation = conv2['conv2_activation']\n",
    "        self.architecture = architecture\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _cnn(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            if conv1_dropout:\n",
    "                conv1 = tf.layers.dropout(conv1, conv1_dropout, training=self._training)\n",
    "\n",
    "        with tf.name_scope(\"pool1\"):\n",
    "            pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            \n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            conv2_fmaps = self.conv2_fmaps\n",
    "            conv2_ksize = self.conv2_ksize\n",
    "            conv2_stride = self.conv2_stride\n",
    "            conv2_pad = \"SAME\"\n",
    "            conv2_dropout = self.conv2_dropout\n",
    "            conv2_activation = self.conv2_activation\n",
    "            conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                     strides=conv2_stride, padding=conv2_pad,\n",
    "                                     activation=conv2_activation, name=\"conv2\")\n",
    "            if conv2_dropout:\n",
    "                conv2 = tf.layers.dropout(conv2, conv2_dropout, training=self._training)\n",
    "\n",
    "        pool2_fmaps = conv2_fmaps\n",
    "        pool2_flat_shape = int(((self.height/2)/2) * ((self.width/2)/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _cnn2(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"pool1\"):\n",
    "            pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            if conv1_dropout:\n",
    "                pool1 = tf.layers.dropout(pool1, conv1_dropout, training=self._training)\n",
    "            \n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            conv2_fmaps = self.conv2_fmaps\n",
    "            conv2_ksize = self.conv2_ksize\n",
    "            conv2_stride = self.conv2_stride\n",
    "            conv2_pad = \"SAME\"\n",
    "            conv2_dropout = self.conv2_dropout\n",
    "            conv2_activation = self.conv2_activation\n",
    "            conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                     strides=conv2_stride, padding=conv2_pad,\n",
    "                                     activation=conv2_activation, name=\"conv2\")\n",
    "\n",
    "        pool2_fmaps = conv2_fmaps\n",
    "        pool2_flat_shape = int(((self.height/2)/2) * ((self.width/2)/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            if conv2_dropout:\n",
    "                pool2_flat = tf.layers.dropout(pool2_flat, conv2_dropout, training=self._training)\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _cnn3(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            if conv1_dropout:\n",
    "                conv1 = tf.layers.dropout(conv1, conv1_dropout, training=self._training)\n",
    "            \n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            conv2_fmaps = self.conv2_fmaps\n",
    "            conv2_ksize = self.conv2_ksize\n",
    "            conv2_stride = self.conv2_stride\n",
    "            conv2_pad = \"SAME\"\n",
    "            conv2_dropout = self.conv2_dropout\n",
    "            conv2_activation = self.conv2_activation\n",
    "            conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                     strides=conv2_stride, padding=conv2_pad,\n",
    "                                     activation=conv2_activation, name=\"conv2\")\n",
    "\n",
    "        pool2_fmaps = conv2_fmaps\n",
    "        pool2_flat_shape = int((self.height/2) * (self.width/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            if conv2_dropout:\n",
    "                pool2_flat = tf.layers.dropout(pool2_flat, conv2_dropout, training=self._training)\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _cnn4(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            if conv1_dropout:\n",
    "                conv1 = tf.layers.dropout(conv1, conv1_dropout, training=self._training)\n",
    "\n",
    "        pool2_fmaps = conv1_fmaps\n",
    "        pool2_flat_shape = int((self.height/2) * (self.width/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        X_reshaped = tf.reshape(X, shape=[-1, self.height, self.width, self.channels])\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "        self._training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "        if self.architecture == 1: #conv1 -> Drop -> max_pool -> conv2 -> Drop -> Max_pool\n",
    "            cnn_outputs = self._cnn(X_reshaped)\n",
    "        if self.architecture == 2: #conv1 -> max_pool -> Drop -> conv2 -> max_pool -> Drop\n",
    "            cnn_outputs = self._cnn2(X_reshaped)\n",
    "        if self.architecture == 3: #conv1 -> Drop-> conv2 -> max_pool -> Drop\n",
    "            cnn_outputs = self._cnn3(X_reshaped)\n",
    "        if self.architecture == 4: #conv -> Drop -> max_pool\n",
    "            cnn_outputs = self._cnn4(X_reshaped)\n",
    "\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            dnn_outputs = self._dnn(cnn_outputs)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"output\")\n",
    "            Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(xentropy)\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            training_op = optimizer.minimize(loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        with tf.name_scope(\"init_and_save\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs=300, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = self.height * self.width * self.channels\n",
    "        n_outputs = np.amax(y_train) + 1\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        self.loss_values = []\n",
    "        self.acc_values = []\n",
    "        self.best_loss_values = []\n",
    "\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        best_loss_val = np.infty\n",
    "        check_interval = 50\n",
    "        checks_since_last_progress = 0\n",
    "        max_checks_without_progress = 30\n",
    "        best_model_params = None \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            if X_valid is not None and y_valid is not None:\n",
    "                for epoch in range(n_epochs):\n",
    "                    rnd_idx = np.random.permutation(len(X_train))\n",
    "                    idx = 0\n",
    "                    for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
    "                        X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "                        X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                        sess.run(self._training_op, feed_dict={self._X: X_batch_reshaped, self._y: y_batch, self._training: True})\n",
    "                        if idx % check_interval == 0:\n",
    "                            X_valid_reshaped = np.reshape(X_valid,(len(X_valid), -1))\n",
    "                            loss_val = self._loss.eval(feed_dict={self._X: X_valid_reshaped,\n",
    "                                                            self._y: y_valid})\n",
    "                            if loss_val < best_loss_val:\n",
    "                                best_loss_val = loss_val\n",
    "                                checks_since_last_progress = 0\n",
    "                                best_model_params = self._get_model_params()\n",
    "                            else:\n",
    "                                checks_since_last_progress += 1\n",
    "                            self.loss_values.append(loss_val)\n",
    "                            self.best_loss_values.append(best_loss_val)\n",
    "                        idx += 1\n",
    "                    X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                    acc_train = self._accuracy.eval(feed_dict={self._X: X_batch_reshaped, self._y: y_batch})\n",
    "                    X_valid_reshaped = np.reshape(X_valid,(len(X_valid), -1))\n",
    "                    acc_val = self._accuracy.eval(feed_dict={self._X: X_valid_reshaped,\n",
    "                                                       self._y: y_valid})\n",
    "                    self.acc_values.append(acc_val)\n",
    "                    print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                              epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
    "                    if checks_since_last_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "\n",
    "                if best_model_params:\n",
    "                    self._restore_model_params(best_model_params)\n",
    "            else: \n",
    "                for epoch in range(n_epochs):\n",
    "                    rnd_idx = np.random.permutation(len(X_train))\n",
    "                    idx = 0\n",
    "                    for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
    "                        X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "                        X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                        sess.run(self._training_op, feed_dict={self._X: X_batch_reshaped, self._y: y_batch, self._training: True})\n",
    "\n",
    "                    X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                    acc_train = self._accuracy.eval(feed_dict={self._X: X_batch_reshaped, self._y: y_batch})\n",
    "                    print(\"Epoch {}, train accuracy: {:.4f}%\".format(\n",
    "                              epoch, acc_train * 100))\n",
    "\n",
    "            return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            X_reshaped = np.reshape(X,(len(X), -1))\n",
    "            return np.argmax(self._Y_proba.eval(feed_dict={self._X: X_reshaped}))\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)\n",
    "\n",
    "    def restore(self, path, n_inputs=960, n_outputs=48):\n",
    "        self.close_session()\n",
    "\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        \n",
    "        self._saver.restore(self._session, path)\n",
    "\n",
    "    def accuracy_score(self, X_test, y_test):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            X_test_reshaped = np.reshape(X_test,(len(X_test), -1))\n",
    "            acc_test = self._accuracy.eval(feed_dict={self._X: X_test_reshaped, self._y: y_test})\n",
    "            print(\"Final accuracy on test set:\", acc_test)\n",
    "\n",
    "            return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train accuracy: 6.4085%, valid. accuracy: 6.6874%, valid. best loss: 3.841489\n",
      "Epoch 1, train accuracy: 17.2230%, valid. accuracy: 17.7294%, valid. best loss: 3.741597\n",
      "Epoch 2, train accuracy: 31.2417%, valid. accuracy: 28.4603%, valid. best loss: 3.515411\n",
      "Epoch 3, train accuracy: 43.1242%, valid. accuracy: 37.0140%, valid. best loss: 3.171391\n",
      "Epoch 4, train accuracy: 47.1295%, valid. accuracy: 40.4355%, valid. best loss: 2.776037\n",
      "Epoch 5, train accuracy: 54.6061%, valid. accuracy: 49.1446%, valid. best loss: 2.459276\n",
      "Epoch 6, train accuracy: 57.9439%, valid. accuracy: 54.2768%, valid. best loss: 2.194318\n",
      "Epoch 7, train accuracy: 62.8838%, valid. accuracy: 55.8320%, valid. best loss: 1.946091\n",
      "Epoch 8, train accuracy: 63.1509%, valid. accuracy: 57.2317%, valid. best loss: 1.797970\n",
      "Epoch 9, train accuracy: 69.2924%, valid. accuracy: 63.7636%, valid. best loss: 1.627434\n",
      "Epoch 10, train accuracy: 72.8972%, valid. accuracy: 65.3188%, valid. best loss: 1.463897\n",
      "Epoch 11, train accuracy: 72.2296%, valid. accuracy: 67.6516%, valid. best loss: 1.372358\n",
      "Epoch 12, train accuracy: 77.8371%, valid. accuracy: 71.0731%, valid. best loss: 1.235450\n",
      "Epoch 13, train accuracy: 83.0441%, valid. accuracy: 73.2504%, valid. best loss: 1.172600\n",
      "Epoch 14, train accuracy: 79.4393%, valid. accuracy: 72.7838%, valid. best loss: 1.092326\n",
      "Epoch 15, train accuracy: 82.9105%, valid. accuracy: 76.2053%, valid. best loss: 1.023712\n",
      "Epoch 16, train accuracy: 86.2483%, valid. accuracy: 76.8274%, valid. best loss: 0.983341\n",
      "Epoch 17, train accuracy: 85.4473%, valid. accuracy: 74.9611%, valid. best loss: 0.954985\n",
      "Epoch 18, train accuracy: 86.7824%, valid. accuracy: 78.2271%, valid. best loss: 0.925927\n",
      "Epoch 19, train accuracy: 87.9840%, valid. accuracy: 78.8491%, valid. best loss: 0.865881\n",
      "Epoch 20, train accuracy: 85.9813%, valid. accuracy: 79.9378%, valid. best loss: 0.840246\n",
      "Epoch 21, train accuracy: 90.9212%, valid. accuracy: 79.0047%, valid. best loss: 0.816575\n",
      "Epoch 22, train accuracy: 88.3845%, valid. accuracy: 78.5381%, valid. best loss: 0.804859\n",
      "Epoch 23, train accuracy: 91.9893%, valid. accuracy: 81.1820%, valid. best loss: 0.770531\n",
      "Epoch 24, train accuracy: 90.7877%, valid. accuracy: 80.4044%, valid. best loss: 0.756416\n",
      "Epoch 25, train accuracy: 91.9893%, valid. accuracy: 80.8709%, valid. best loss: 0.742624\n",
      "Epoch 26, train accuracy: 92.1228%, valid. accuracy: 80.2488%, valid. best loss: 0.725188\n",
      "Epoch 27, train accuracy: 92.9239%, valid. accuracy: 81.4930%, valid. best loss: 0.699908\n",
      "Epoch 28, train accuracy: 92.9239%, valid. accuracy: 81.8040%, valid. best loss: 0.696925\n",
      "Epoch 29, train accuracy: 93.9920%, valid. accuracy: 81.3375%, valid. best loss: 0.677758\n",
      "Epoch 30, train accuracy: 93.4579%, valid. accuracy: 82.4261%, valid. best loss: 0.677758\n",
      "Epoch 31, train accuracy: 94.7931%, valid. accuracy: 81.9596%, valid. best loss: 0.663908\n",
      "Epoch 32, train accuracy: 93.4579%, valid. accuracy: 82.5817%, valid. best loss: 0.660493\n",
      "Epoch 33, train accuracy: 94.1255%, valid. accuracy: 82.7372%, valid. best loss: 0.652293\n",
      "Epoch 34, train accuracy: 95.0601%, valid. accuracy: 82.4261%, valid. best loss: 0.633446\n",
      "Epoch 35, train accuracy: 95.7276%, valid. accuracy: 82.2706%, valid. best loss: 0.624232\n",
      "Epoch 36, train accuracy: 95.8611%, valid. accuracy: 82.7372%, valid. best loss: 0.624232\n",
      "Epoch 37, train accuracy: 96.1282%, valid. accuracy: 82.1151%, valid. best loss: 0.624232\n",
      "Epoch 38, train accuracy: 95.7276%, valid. accuracy: 82.8927%, valid. best loss: 0.605367\n",
      "Epoch 39, train accuracy: 95.8611%, valid. accuracy: 82.5817%, valid. best loss: 0.605367\n",
      "Epoch 40, train accuracy: 97.1963%, valid. accuracy: 82.4261%, valid. best loss: 0.605367\n",
      "Epoch 41, train accuracy: 98.5314%, valid. accuracy: 83.0482%, valid. best loss: 0.605367\n",
      "Epoch 42, train accuracy: 96.5287%, valid. accuracy: 81.9596%, valid. best loss: 0.605367\n",
      "Epoch 43, train accuracy: 97.5968%, valid. accuracy: 82.5817%, valid. best loss: 0.605367\n",
      "Epoch 44, train accuracy: 97.3298%, valid. accuracy: 83.0482%, valid. best loss: 0.567946\n",
      "Epoch 45, train accuracy: 96.7957%, valid. accuracy: 83.6703%, valid. best loss: 0.567946\n",
      "Epoch 46, train accuracy: 98.5314%, valid. accuracy: 83.5148%, valid. best loss: 0.567946\n",
      "Epoch 47, train accuracy: 97.1963%, valid. accuracy: 82.1151%, valid. best loss: 0.565041\n",
      "Epoch 48, train accuracy: 97.4633%, valid. accuracy: 81.3375%, valid. best loss: 0.565041\n",
      "Epoch 49, train accuracy: 97.9973%, valid. accuracy: 83.0482%, valid. best loss: 0.565041\n",
      "Epoch 50, train accuracy: 98.7984%, valid. accuracy: 83.2037%, valid. best loss: 0.551559\n",
      "Epoch 51, train accuracy: 97.3298%, valid. accuracy: 82.8927%, valid. best loss: 0.551559\n",
      "Epoch 52, train accuracy: 98.9319%, valid. accuracy: 83.5148%, valid. best loss: 0.551559\n",
      "Epoch 53, train accuracy: 98.1308%, valid. accuracy: 83.5148%, valid. best loss: 0.551559\n",
      "Epoch 54, train accuracy: 99.4660%, valid. accuracy: 83.6703%, valid. best loss: 0.545954\n",
      "Epoch 55, train accuracy: 98.5314%, valid. accuracy: 84.1369%, valid. best loss: 0.544680\n",
      "Epoch 56, train accuracy: 98.9319%, valid. accuracy: 84.2924%, valid. best loss: 0.544680\n",
      "Epoch 57, train accuracy: 98.2644%, valid. accuracy: 84.2924%, valid. best loss: 0.540390\n",
      "Epoch 58, train accuracy: 98.6649%, valid. accuracy: 82.7372%, valid. best loss: 0.540390\n",
      "Epoch 59, train accuracy: 99.3324%, valid. accuracy: 84.2924%, valid. best loss: 0.540390\n",
      "Epoch 60, train accuracy: 99.1989%, valid. accuracy: 84.4479%, valid. best loss: 0.527209\n",
      "Epoch 61, train accuracy: 98.3979%, valid. accuracy: 84.4479%, valid. best loss: 0.527209\n",
      "Epoch 62, train accuracy: 98.7984%, valid. accuracy: 84.1369%, valid. best loss: 0.523702\n",
      "Epoch 63, train accuracy: 99.3324%, valid. accuracy: 83.9813%, valid. best loss: 0.523702\n",
      "Epoch 64, train accuracy: 99.0654%, valid. accuracy: 83.6703%, valid. best loss: 0.523702\n",
      "Epoch 65, train accuracy: 98.9319%, valid. accuracy: 84.1369%, valid. best loss: 0.523702\n",
      "Epoch 66, train accuracy: 98.6649%, valid. accuracy: 82.7372%, valid. best loss: 0.523702\n",
      "Epoch 67, train accuracy: 99.1989%, valid. accuracy: 83.3593%, valid. best loss: 0.523702\n",
      "Epoch 68, train accuracy: 99.0654%, valid. accuracy: 85.5365%, valid. best loss: 0.523702\n",
      "Epoch 69, train accuracy: 99.1989%, valid. accuracy: 84.6034%, valid. best loss: 0.523702\n",
      "Epoch 70, train accuracy: 99.3324%, valid. accuracy: 84.9145%, valid. best loss: 0.523702\n",
      "Epoch 71, train accuracy: 99.4660%, valid. accuracy: 84.4479%, valid. best loss: 0.523702\n",
      "Epoch 72, train accuracy: 99.4660%, valid. accuracy: 84.2924%, valid. best loss: 0.523702\n",
      "Epoch 73, train accuracy: 99.5995%, valid. accuracy: 84.4479%, valid. best loss: 0.515005\n",
      "Epoch 74, train accuracy: 98.6649%, valid. accuracy: 85.5365%, valid. best loss: 0.515005\n",
      "Epoch 75, train accuracy: 99.4660%, valid. accuracy: 85.2255%, valid. best loss: 0.515005\n",
      "Epoch 76, train accuracy: 99.7330%, valid. accuracy: 84.6034%, valid. best loss: 0.515005\n",
      "Epoch 77, train accuracy: 99.1989%, valid. accuracy: 83.3593%, valid. best loss: 0.515005\n",
      "Epoch 78, train accuracy: 99.5995%, valid. accuracy: 83.8258%, valid. best loss: 0.515005\n",
      "Epoch 79, train accuracy: 99.4660%, valid. accuracy: 84.6034%, valid. best loss: 0.515005\n",
      "Epoch 80, train accuracy: 99.4660%, valid. accuracy: 84.4479%, valid. best loss: 0.515005\n",
      "Epoch 81, train accuracy: 99.8665%, valid. accuracy: 84.7589%, valid. best loss: 0.515005\n",
      "Epoch 82, train accuracy: 99.3324%, valid. accuracy: 84.4479%, valid. best loss: 0.515005\n",
      "Epoch 83, train accuracy: 99.7330%, valid. accuracy: 85.2255%, valid. best loss: 0.515005\n",
      "Epoch 84, train accuracy: 99.4660%, valid. accuracy: 83.9813%, valid. best loss: 0.515005\n",
      "Epoch 85, train accuracy: 99.4660%, valid. accuracy: 83.9813%, valid. best loss: 0.515005\n",
      "Epoch 86, train accuracy: 99.4660%, valid. accuracy: 85.2255%, valid. best loss: 0.515005\n",
      "Epoch 87, train accuracy: 99.7330%, valid. accuracy: 84.6034%, valid. best loss: 0.515005\n",
      "Epoch 88, train accuracy: 100.0000%, valid. accuracy: 84.1369%, valid. best loss: 0.515005\n",
      "Epoch 89, train accuracy: 99.7330%, valid. accuracy: 84.6034%, valid. best loss: 0.515005\n",
      "Epoch 90, train accuracy: 99.4660%, valid. accuracy: 84.9145%, valid. best loss: 0.515005\n",
      "Epoch 91, train accuracy: 99.7330%, valid. accuracy: 84.2924%, valid. best loss: 0.515005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, train accuracy: 99.7330%, valid. accuracy: 83.8258%, valid. best loss: 0.515005\n",
      "Epoch 93, train accuracy: 99.8665%, valid. accuracy: 83.3593%, valid. best loss: 0.515005\n",
      "Epoch 94, train accuracy: 99.5995%, valid. accuracy: 85.2255%, valid. best loss: 0.515005\n",
      "Epoch 95, train accuracy: 99.1989%, valid. accuracy: 84.9145%, valid. best loss: 0.515005\n",
      "Epoch 96, train accuracy: 99.7330%, valid. accuracy: 83.9813%, valid. best loss: 0.515005\n",
      "Epoch 97, train accuracy: 100.0000%, valid. accuracy: 84.2924%, valid. best loss: 0.515005\n",
      "Epoch 98, train accuracy: 99.5995%, valid. accuracy: 84.4479%, valid. best loss: 0.515005\n",
      "Epoch 99, train accuracy: 99.7330%, valid. accuracy: 84.7589%, valid. best loss: 0.515005\n",
      "Epoch 100, train accuracy: 99.8665%, valid. accuracy: 85.3810%, valid. best loss: 0.515005\n",
      "Epoch 101, train accuracy: 99.7330%, valid. accuracy: 83.3593%, valid. best loss: 0.515005\n",
      "Epoch 102, train accuracy: 99.7330%, valid. accuracy: 84.1369%, valid. best loss: 0.515005\n",
      "Epoch 103, train accuracy: 99.5995%, valid. accuracy: 85.3810%, valid. best loss: 0.515005\n",
      "Epoch 104, train accuracy: 99.8665%, valid. accuracy: 84.6034%, valid. best loss: 0.515005\n",
      "Early stopping!\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "cnnClassifier = CNNClassifier(n_hidden_layers=2, n_neurons=400, optimizer_class=tf.train.AdamOptimizer,\n",
    "                              learning_rate=0.01, batch_size=600, activation=leaky_relu(), dropout_rate=0.1,\n",
    "                              conv1={'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':0.3, 'conv1_activation':tf.nn.relu},\n",
    "                              conv2={'conv2_fmaps':16, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.3, 'conv2_activation':tf.nn.relu},\n",
    "                              architecture=1, height=12, width=80)\n",
    "cnnClassifier.fit(X_train, y_train, n_epochs=300, X_valid=X_valid, y_valid=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test set: 0.82298136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82298136"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnClassifier.accuracy_score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAELCAYAAADX3k30AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15c5a3e9e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnClassifier.best_loss_values)\n",
    "plt.title('Best losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAELCAYAAADX3k30AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJwvZEwgJO2FHEARUCqJYqeXWqnWp3FrF\nam21ru213trWe2/trz/a321tbetta229t62iYsXWpS5F7a1WUVSCArJvkgRIIBvZ93x/f5wTHOKE\nDDBkkpP38/GYh5nvfOfM5zvE93zzPWfOMeccIiISXHGxLkBERE4sBb2ISMAp6EVEAk5BLyIScAp6\nEZGAU9CLiAScgl6kjzKzPDOrNbP4aPaV4FHQS5fM7FUzqzSzpFjXEjRmdq2ZrTyebTjnCp1z6c65\ntmj2leBR0EtYZjYWOBtwwMU9/NoJPfl6J9qxjkezb4kWBb105RrgLeBB4IuhD5hZipn91MwKzKzK\nzFaaWYr/2Hwze9PMDppZkZld67e/ambXh2zjsBmtmTkzu9XMtgPb/bb/8rdRbWZrzOzskP7xZvbv\nZrbTzGr8x0eb2X1m9tNO9f7FzG4PN0gzO9PMVvvjWG1mZ/rtnzez/E59bzezv/g/J5nZPWZWaGb7\nzew3Ie/BAjPbY2bfNrMS4A+dtjMV+A0wz19OOei3P2hm95vZC2ZWB3zCzC40s/f896DIzL4Xsp2x\n/vuWEPIef9/M3vDfk5fMLOdo+/qPX+P/+5ab2V1mttvMFoZ7D6UPcM7ppttHbsAO4BbgdKAFGBry\n2H3Aq8BIIB44E0gCxgA1wJVAIjAYmOU/51Xg+pBtXAusDLnvgJeBbCDFb/uCv40E4BtACZDsP/ZN\n4H3gJMCAmX7fOcA+IM7vlwPUh9Yf8prZQCVwtf8aV/r3BwOp/lgmhfRfDVzh//xz4C/+NjKAZ4Ef\n+o8tAFqBu/33JSXMax82fr/tQaAKOAtvEpbsb+sU//4MYD9wqd9/rP++JYS8xzuByUCKf/9Hx9D3\nZKAWmA8MAO7B+x1YGOvfS92O8f/nWBegW++7+f+DtwA5/v0twO3+z3FAAzAzzPP+DXiqi22+SvdB\nf243dVV2vC6wFbiki36bgX/yf/4q8EIX/a4G3unUtgq41v/5EeC7/s+T/OBPxftgqQMmhDxvHvCB\n//MCoBn/Q6mL1+4q6Jd28x7cC/zc/zlceH8npO8twIpj6Ptd4LGQx1L98Sjo++hNSzcSzheBl5xz\nZf79ZXy4fJODN9PcGeZ5o7toj1RR6B0zu8PMNvvLKgeBLP/1u3uth/D+GsD/78Nd9BsBFHRqK8D7\nSwW8cV/p/7wYeNo5Vw/k4oXfGn+J6iCwwm/vUOqca+zidY+k83sw18xeMbNSM6sCbuLD9yCckpCf\n64H0Y+g7IrQOf8zlEdQuvZSCXg7jrzNfDpxjZiX+GvPtwEwzmwmUAY3AhDBPL+qiHbwZcGrI/WFh\n+hw6laq/Hv8tv5ZBzrmBeMsaFsFrPQJc4tc7FXi6i3778JabQuUBe/2fXwZyzWwWXuAv89vL8P6q\nmeacG+jfspxzoaHa3Wlhu3q8c/syvCWi0c65LLy1ffvIs6KrGBjVccf/nRh8gl9TTiAFvXR2KdCG\nt047y79NBV4HrnHOtQO/B35mZiP8naLz/EMwHwUWmtnlZpZgZoP9kARYC1xmZqlmNhG4rps6MvDW\nuUuBBDP7LpAZ8vj/AN83s0nmmWFmgwGcc3vw1tMfBv7snGvo4jVeACab2WK/3s/7437O304L8ATw\nE7y1+Jf99nbgv4Gfm9kQADMbaWbndTOmUPuBUWY2IIL3ocI512hmc/D+sjjR/gRc5O+oHgB8jxP/\n4SInkIJeOvsi8AfnHXdd0nEDfgVc5R+1cQfejtDVQAXeTsc451whcAHejtMKvHCf6W/353jrvPvx\nllYe7aaOF/GWQ7bhLac0cviyxs+A5cBLQDXwO7ydih0ewtuJ2dWyDc65cuAzfr3leH9BfCZkyQq8\nGfVC4AnnXGtI+7fxdli/ZWbVwN/wdgxH6u/ARqDEzMqO0O8WYImZ1eCtnS8/itc4Js65jcDXgD/i\nze5rgQNA04l+bTkxzDldeESCx8w+jreEM8bpl/y4mFk6cBDvCKQPYl2PHD3N6CVwzCwRuA34H4X8\nsTGzi/xltjS8wyvfB3bHtio5Vgp6CRT/y0gHgeF4hyLKsbkEb2f1PrxDS6/Qh2bfpaUbEZGA04xe\nRCTgesXJo3JyctzYsWNjXYaISJ+yZs2aMudcbnf9ekXQjx07lvz8/O47iojIIWbW+ZvdYWnpRkQk\n4BT0IiIBp6AXEQk4Bb2ISMAp6EVEAk5BLyIScAp6EZGAU9CLBFBtUyvL3i6kvrm1+85H8Nauch5f\nXUhRRX2UKgu21rZ2mlrbjtinsaWN9vaePfVMr/jClIhE1z0vbuXBN3ezdNVuHrh6NnmDU7t9TmeP\nvl3Ad57eQMfpsEZnp/DZWSO5/uPjyUxOjGq9z68v5h/bDvCdz5zc5baXry5iw74qbjxnAiMHpoTt\nc6wOVDey5LlNTBySztmTcpgxaiCJ8ZHPg1vb2nnyvb3c+/I2MpITefZr8xmQ8NHnf1BWx+d/u4rs\ntAHc87mZTB+ZFc1hdKlXnNRs9uzZTt+MFYmOoop6zv3pq8wek82m4moA/uOCqew92MDKHWUcqGnk\n4pkjuOJjeYzODv8B8OtXd/DjFVv55JQh3HHeSbzzQQWvbD3Aq1tLGZiayK0LJnL1vDEkJ8Z3WceB\n6kaSB8QfCu6D9c28ubOcLcXVTBuZxbwJg2lubee7z2zghfe9y9d+bOwgln55LikDDt/u8tVFfOvP\n6wEYkBDHNWeM4eYFExicnnRYv7LaJl7fXsrK7eWs3l3B1OEZ3PGpk5g0NOOI79nNj6zhpU37aXcO\n52BgaiI/umwGn54e7oqXHuccu8vrWbm9lIdWFbDjQC0TctPYWVrHdy6cyvVnjz+s/76DDXzuN6to\naGkjIc4or2vm1gUT+Oq5k8J+KETCzNY452Z3209BL3J8Glva2FPZQFFFPaU1TTj/sq/jctKZMy77\nsL5FFfWs2FBCYUU9hRX1VDe2HHps1KBU5k8czFkTcxg16Ohn4B1uf3wtL7xfzD+++QmaWtu48eE1\nbCmpIc7glFEDyUpJZOX2UhxwWt4gJuSmkZedSlJCPEWV9WzfX8uqXeVcMmsE93xu5mEz2w17q7h7\nxRZe317GnHHZLP3ynI+EfV1TK3ev2MLSVd638wemJpKdNoAPyuoIjZs4g5TEeFraHLctnMSoQSnc\n/vhazp6Uy39fM/tQ+D2/vpivPfYuZ03MYckl07nvlR08+e4eEuLjuGD6MK6ck0dTazvL3i7kb5v3\n09ruGJSayOljBvHWrgrqm1tZdNoobl4wgfG5H71W+t827ef6pfl887yTWDwnj1W7yvnta7tYV3SQ\nr507ka8vnHxo7GuLDlJQ7v3bbS6uZu9B7yqVk4ak841PTea8acO49g+rebewklfvWHDog6istonL\nf7uK0uomHrvhDEYPSuX/PreRJ9/dyxfOyOMHl55yTP/WCnqRE6i1rZ2/bznAY+8U8o9tpXS15PrQ\nl+dwzmTvnFOVdc1c8IvXKa5qJCslkbzsVAamfrhMsbWkhgM13tX6BqV6j4/OTmXW6IGcNTGHk4Zm\nsHFfNcveKWDFhhK+ed4UFs/NO+z1Nu2r5sJfvs5N50zg25+eAkBDcxtriw5y8vBMsvzX23ewgcdX\nF/HGjjIKK+oPvW5GcgJjBqdy7klD+PrCycTFhb9U7FPv7eFfl69jweRcfnu1F8rOOd7cWc6dT65n\nT2UDV58xhlGDUiisqGd/dRPTRmRy9qQcpg7P5P09VazcUcbeygZuWjCByf6M+/HVhXz7z+8ze8wg\nJuSm09ru+Mu6vcwcNZCl180hdYC32rzjQA1LVxXw1Lt7qWny9kNkpw3gn08fxUUzRjBtRCZxcUZF\nXTP3vbKDh1cV0NzWzrzxg7lybh7nTRtKUkI8dU2tfOrnr5E6IJ7n/+XsQx8uTa1tfPfpjTyeX8SU\nYRkUVzVS1eB9KKckxpOXncqEIWmcOSGH+RNzGDM4FTM7VNun732dyz82mv/87Cm8V1jJN5avY19V\nAw9fN5ePjf3ww/9vm/YzZXjGMX+wK+ilXympauQXf9/O5uJqRg1KZUx2KudOHcJpeYOOeZt1Ta08\nt34ftU3ezrXWtnb2HWygoKKejfuqKa1pYkhGEp89dSRThmeQl53KkIxk4uOMtnbHV5bmc6Cmief/\nZT5DM5K5fmk+K7eX8fiNZ3BqmLqcc+w4UMsbO8rYdqCWoop6PiirY0+lN2tMT0qgtqmV5MQ4Rg5M\nYXd5PQ99aQ7zJ+Uc2sa1f3iH9woP8tq3PkFWSuTr6A3NbTS3th/6IIjEsrcL+fen3uczM4Yzb8Jg\nlr1dyMZ91YwdnMqP/3nmR/6aidTSVbv57T920eZ/ek4ams6vFp8Wdjz1za28uLGEAfHxLDx5CEkJ\n4ZeSDtQ08kT+Hh57p5A9lQ2HPhQO1jezPH8PT9w077AABu/f49G3C1n2diHTRmQyf1IOc8ZlMywz\n+VCod+X/PruRB9/czeWnj+aJNUUMy0zmZ5+fxRnjBx/Te9IVBb30ec2t7ewsrWXq8Mwu+5TXNvHA\n67t48I3dtDvHqXmDKKlqZO/BBhLijMdvnMes0QPDPregvI783ZUUVtSz92ADedmpzJ+Uw9RhmSzP\nL+KXf99OWW3zYc/pmPGOy0nnohnDOXfKEBK62Gm3s7SWi365kmkjMllw0hB+8uJWllwyjWvmjT2q\n96G4qoE3dpSTv7uCaSMyueTUkRiw6P43Kalq5OlbzyIpMZ6fv7yNP63Zw7+dP4Ubz5lwVK9xrH77\nj5388K9bAJgyLIPFc/P43OmjP7LG3lu0tzte31HGYyHLPFfOGc0PL5sR1depqm9hwT2vUFnfwhUf\nG82/Xzg16juwQUEvJ1BLWzsflNVR5K8zpyUlcNbEnLBHQlTWNfPy5v1MGZbBjFEfBu4rWw9w79+2\nMyIrmbMm5nDmhMGMGZxGfJzR3u74y7p9/PTlrRRVNHDzggl867yTDs2inHOs2lXOsrcLeXFjCa3t\njs+eOpLbF04+tHOxvLaJS3/9Bo0t7fzlq2cxPOvw2naV1nLhL1bS0NKGGeSkJ1FW24Rz3tpxu4O5\n47L55nkf7siLjzPSBsR3O5sL9fR7e/n642sBuPCU4fxq8alH9fwjKaqo55L73iA+zqiqbwGDL84b\nwzfPm3LMO/eOxYsbS8jNSOLU0QOjNraecKC6kX9sK+WCU4aTlhT9AxA37K2ioaXtI38pRJOCXo7b\nixtL+Num/Xz2tJHMGz+YducF189e3nZoJ1So8TlpzBiVRV52KiMGpvDWrnJe2FBCc2s7ABecMowb\nPz6BR94q4Ik1exgzOJWW1nb2VTUCMCA+jlGDUmj3j2Y4eXgmE4ak8+y6fVw1N48ll0xn1c5y7l6x\nhff3VpGVkshlp43kqrl5TBzy0aMqtu2v4bJfv8nYnFSW3zjv0Ppua1s7n/vtKnaV1rHsK3OZOCSd\npIR4KuqaeXNnGWsLDzJ/Ug7nTM6NSnB9/7lNvLWrnMduOCPqs7rVuyu48eE1LJw6hNsWTo76YYfS\nuynoJWKtbe1UNbQcdqjaY+9466+GN7sdl5NGYryxbX8t00dmcu2Z45iQm8bo7FQq6pp5fXsZb+wo\nY2tJDcVVDbQ7yEhK4LOnjeSy00bx6tYD/Pdru6hrbiPO4KZzJnDbwkkMiI9jd3k9b+8qZ3d5PUUV\n9VTWN/P5j43mohkjMIOfvLiVX7+6k1GDUthT2cDIgSn8yycncsmskUc8vA/g71v2c91D+cwdl81P\nL5/FyIEphw4d/K8rZnHJrJEn+N31OOf61GxX+gYFvRxRU2sbT+Tv4bVtpazaVU5NYytzxmVz1dw8\n9h1s5O4VWzhnci73fn4Wr247wGNvF1HT1Mqtn5jABdOHd3k0Bnhr68VVDQzJSD5srbastomn39vL\nnHHZhy3jROKB13aydFUBXzprHF84I6/LnW7h/HnNHu56ZgNxZlx/9jjue2UHnzp5WFSXUURiQUHf\nzzjneOjN3UwbmdXtmmBLWzs3P/Iuf9u8n5EDUzh7Ug5DM5N5eu1eCsq9r7p/ZsZwfnb5rB5d6z2R\niirq+daf1rNqVzk56Um8dPvHyU4bEOuyRI5LVIPezLKB3wGfAsqAf3POLQvTz4DvA18C0oH3gFud\ncxuPtH0F/fF7e1c5n3/gLczg2jPH8q3zptDY0saT7+3l1a0HWDh1KFfOySMhzvjX5Wt5eu0+llwy\njavPGHNoVtve7u3k3FVWx+I5ecQfYdbeF7W3O55dv4/xOemcMqpnvnouciJFO+gfwzsB2nXALOB5\n4MzOAW5mlwP3AvOBAuAHwHnOudOOtH0F/fG7YWk+q3dXcNHMESxdVcDQzCQq61tobm1nRFYy+6oa\nGZ2dwtRhmby0aT/fPO8kbv3ExFiXLSLHIdKg7/aYIjNLAxYB051ztcBKM3sGuBq4s1P3ccBK59wu\n/7mPALcfbfHyUW/uLCMlMT7sF20Kyut4efN+bl0wkTvOO4nzpw/nv/53G+cNzeCKj+UxdXgGr20v\n48crtvDSpv3ceM54blnQM8dZi0jsRXLw6GSg1Tm3LaRtHbAgTN8/Apeb2WTgA+CLwIpwGzWzG4Ab\nAPLy8sJ1EV/HWQTjzPi386dw3fxxh+1E/MMbu0mIM66ZNwaAeRMGM2/CvMO2cc7kXM6emMOO0lom\nDUnXTkiRfiSSoE8Hqju1VQPhTgdXDKwEtgJtQBFwbriNOuceAB4Ab+kmwnr7nY5DAT9xUi5JCfH8\n4PnNbNhbxQ8vm0HKgHiqG1t4Ir+Ii2aMYEhm8hG3FRdnh84pIiL9RyRBXwt0/g56FlATpu93gTnA\naKAE+ALwdzOb5pzTlQuOQlltEz99aSuPvVPExTNH8NPLZxJvxq9f3cFPX97Ga9vL+Nzpo2h3jrrm\nNr48f1ysSxaRXiqSoN8GJJjZJOfcdr9tJhDuSJpZwB+dc3v8+w+a2b3AyYD2tkagrqmVB17bxf+8\nvouGlja+cvY47jx/6qEjYL567iTmjBvM71d+wP+s/IC2dsfccdk9dgEDEel7ug1651ydmT0JLDGz\n64FTgYuBM8N0Xw18zsz+CJQCVwGJwI7olRxsdz75Ps+u28f504fxjU+dxMQhHz1/9pxx2cwZl82B\n6kaef7+Ys0POXigi0lmkZ/K5Bfg9cAAoB252zm00szxgE3Cyc64QuBsYAqwF0vACfpFz7mDUKw+g\nqvoWXtxQwrVnjuV7F0/rtv+QzGS+dJaWbETkyCIKeudcBXBpmPZCvJ21HfcbgVv9mxylFzYU09zW\nzmWn9cz5V0SkfwjG99v7mNa2dpbnF3HPi1sPuxr80+/tZXxuGqdovV1Eoij6J2GWLjnneGnTfn7y\n4lZ2HKgFYNSgFK6Yk8fegw28/UEF//pPk3WMu4hElWb0PeiPq4u48eE1tDvH/Vedxtxx2fznC5sp\nrWniL2v3AXBpD502V0T6D83oe9AT/oWGn/vafBLi45g0NIML/ut1/t/zm9hcXMPpYwaRN/jYLhIs\nItIVzeh7yJ7Ket4tPMhFM0ccusboxCHp3LxgAk+v3cfW/TVcOmtEjKsUkSBS0PeQ59cXA3DRjMPD\n/OYFExifk0ZCnHHhDAW9iESflm56yHPri5k5KusjSzPJifE8cM3p7Cyt04UwROSE0Iy+B+wuq+P9\nvVV8posZ+8QhGZw3bVgPVyUi/YWCvgc8t947oubCGcNjXImI9EcK+hOgpKqRbyxfxzNr99LU2sZz\n64s5fcwgRgxMiXVpItIPaY3+BLh7xRaeem8vf353D1kpiVQ1tPC9i06OdVki0k8p6KNsc3E1T6/d\ny40fH8/Zk3J57J1CNhVX64gaEYkZBX2U/XjFFjKSErhlwUSyUhOZr1MIi0iMaY0+it7aVc4rW0u5\n5RNeyIuI9AYK+ihxzvGjv25hWGYy1545NtbliIgcElHQm1m2mT1lZnVmVmBmi7vo9xszqw25NZlZ\nuGvLBs6qXeWsLTrIbQsnkZwYH+tyREQOiXSN/j6gGRiKd13Y581snXPusOvGOuduAm7quG9mDwLt\n0Sm1d3t2XTGpA+J19kkR6XW6ndGbWRqwCLjLOVfrnFsJPANcHeHzHopGob1ZS1s7KzYUs3DqUFIG\naDYvIr1LJEs3k4FW59y2kLZ1QHcXNV2Ed4Hw18I9aGY3mFm+meWXlpZGVGxv9caOMirrW7hopg6h\nFJHeJ5KgTweqO7VVAxndPO+LwFLnnAv3oHPuAefcbOfc7Nzc3AjK6L2eW19MRnICH5+sQylFpPeJ\nJOhrgcxObVlAlztZzSwPWAAsPebK+oim1jZe3FjCp04eRlKClm1EpPeJJOi3AQlmNimkbSawsYv+\n4K3fv+Gc23U8xfUFr20ro6axlYtm6oRlItI7dRv0zrk64ElgiZmlmdl84GLg4SM87RrgwahU2Ms9\nt34fg1ITOWuilm1EpHeK9AtTtwApwAFgGXCzc26jmeX5x8vndXQ0s3nAKOCJqFfbyzQ0t/Hypv18\nevowEuP13TMR6Z0iOo7eOVcBXBqmvRBvZ21o2yogLSrV9XKPvl1AfXMbi04bFetSRES6pGnoMWpo\nbuM3/9jFmRMGM3tsdqzLERHpkoL+GD36dgFltU3c9slJ3XcWEYkhBf0xCJ3Nzx0/ONbliIgckYL+\nGGg2LyJ9iYL+KFXVt2g2LyJ9ioL+KDjnuONP6zhY38yd50+JdTkiIhFR0B+F37+xm5c37efO86cw\nY9TAWJcjIhIRBX2E1hYd5Ed/3czCqUO5bv64WJcjIhIxBX0EnHPc9sf3GJKRzD2fm4GZxbokEZGI\nKegjsKeygYLyem5eMIGBqQNiXY6IyFFR0EdgS4l3RuaTR3Q+W7OISO+noI/AlmLvuiuTh3Z3rRUR\nkd5HQR+BLSU15GWnkp4U6bXURUR6DwV9BLaUVDNlmGbzItI3Kei70djSxgdldUwZrvV5EembIgp6\nM8s2s6fMrM7MCsxs8RH6jjez58ysxszKzOzH0Su3523fX0u7QzN6EemzIp3R3wc0A0OBq4D7zWxa\n505mNgB4Gfg7MAzvSlOPRKfU2Nhc4u2IVdCLSF/VbdCbWRqwCLjLOVfrnFsJPIN3AfDOrgX2Oed+\n5pyrc841OufWR7XiHra1pIbkxDjGDO4XF80SkQCKZEY/GWh1zm0LaVsHfGRGD5wB7Dazv/rLNq+a\n2SnhNmpmN5hZvpnll5aWHn3lPWRLSTUnDc0gPk7fhhWRvimSoE8Hqju1VQPh1jJGAVcAvwBGAM8D\nz/hLOodxzj3gnJvtnJudm5t7dFX3EOccm4trOEnLNiLSh0US9LVA50NOsoCaMH0bgJXOub8655qB\ne4DBwNTjqjJGSmubqKhrZsowHXEjIn1XJEG/DUgws9DLKc0ENobpux5w0SisN9jqn/pgynDN6EWk\n7+o26J1zdcCTwBIzSzOz+cDFwMNhuj8CnGFmC80sHvg6UAZsjmLNPWZLsR/0mtGLSB8W6eGVtwAp\nwAFgGXCzc26jmeWZWa2Z5QE457YCXwB+A1QClwAX+8s4fc7mkmqGZCSRnaYzVopI3xXRyVuccxXA\npWHaC/F21oa2PYn3F0Cft7m4Rt+IFZE+T6dA6MKytwvZXFzNmRN0AXAR6dsU9GG8ubOM7z6zgQUn\n5XK9LhsoIn2cgr6T3WV13PLou4zNSeMXV55KQrzeIhHp25RiIeqbW/nK0nwM+N0XZ5OZnBjrkkRE\njpuupBHirqc3sqO0lkeum6tz24hIYGhG71ueX8Sf393DbZ+cxFkTc2JdjohI1Cjo8b4B+91nNnDm\nhMF87dxJ3T9BRKQPUdAD3/vLRtKTErn3ilk6S6WIBE6/D/qm1jbWFFRy2WkjGZKRHOtyRESirt8H\n/Ya91TS3tXP6mEGxLkVE5ITo90H/bkElAKflKehFJJj6fdDnF1QwZnAquRlJsS5FROSE6NdB75xj\nTcFBTtdsXkQCrF8HfVFFA2W1TZym9XkRCbB+HfRrCisAmD1WQS8iwRVR0JtZtpk9ZWZ1ZlZgZou7\n6HetmbX5FyPpuC2IasVRtKagkoykBCYN0aUCRSS4Ij3XzX1AMzAUmAU8b2brnHPhrhu7yjk3P1oF\nnkj5uyuZlTdQX5ISkUDrdkZvZmnAIuAu51ytc24l8Axw9Yku7kSqaWxh6/4aHT8vIoEXydLNZKDV\nObctpG0dMK2L/qeaWZmZbTOzu8ws7F8NZnaDmeWbWX5paelRln381hYdxDmYPSa7x19bRKQnRRL0\n6UB1p7ZqINzC9mvAdGAI3l8BVwLfDLdR59wDzrnZzrnZubm5kVccJWsKKokzmDk6q8dfW0SkJ0US\n9LVA5ytkZwE1nTs653Y55z5wzrU7594HlgD/fPxlRt/6PVVMHppBhi4uIiIBF0nQbwMSzCz0/L0z\ngXA7YjtzQK/c07m7vI7xubq4iIgEX7dB75yrA54ElphZmpnNBy4GHu7c18zON7Oh/s9TgLvwdtz2\nKu3tjj0VDYzOTo11KSIiJ1ykX5i6BUgBDgDLgJudcxvNLM8/Vj7P7/dJYL2Z1QEv4H1A/Ge0iz5e\nJdWNNLe1MyZbM3oRCb6IjqN3zlUAl4ZpL8TbWdtx/w7gjqhVd4IUVtQDkKcZvYj0A/3yFAgKehHp\nT/pn0JfXEx9nDB+oK0qJSPD1z6CvqGfkwBQS4/vl8EWkn+mXSVdYUa9lGxHpN/pt0OvQShHpL/pd\n0Nc0tlBR18yYwQp6Eekf+l3QF1U0ADriRkT6j34X9IUVdYCCXkT6j34Y9P4x9Fq6EZF+ol8G/cDU\nRDJ11koR6Sf6XdAXlOvQShHpX/pd0BfpGHoR6Wf6VdC3tTv2VDYo6EWkX+lXQb/vYAOt7U5BLyL9\nSr8K+iKdtVJE+qGIgt7Mss3sKTOrM7MCM1scwXP+18ycmUV0zvueoEMrRaQ/ijSE7wOagaHALOB5\nM1vnnAta0ngrAAANnUlEQVR73VgzuwrodccvFlTUkxBnDM9KiXUpIiI9ptsZvZmlAYuAu5xztc65\nlXjXgb26i/5ZwP8BvhXNQqNhXdFBxuWkER/XK69XLiJyQkSydDMZaHXObQtpWwdM66L/fwL3AyVH\n2qiZ3WBm+WaWX1paGlGxx6O0pom3dpXz6enDTvhriYj0JpEEfTpQ3amtGsjo3NHMZgNnAb/sbqPO\nuQecc7Odc7Nzc3MjqfW4/HVDMe0OPjNjxAl/LRGR3iSSoK8FMju1ZQE1oQ1mFgf8GrjNOdcanfKi\n59l1+5g8NJ2Thn3k80lEJNAiCfptQIKZTQppmwl03hGbCcwGHjezEmC1377HzM4+7kqPQ3FVA6t3\nV2o2LyL9UrdH3Tjn6szsSWCJmV0PnApcDJzZqWsVEJqko4F3gNOBE78IfwTPry8G4DMzhseyDBGR\nmIj0C1O3ACnAAWAZcLNzbqOZ5ZlZrZnlOU9Jx40Pw32/c675BNQesWfXFzNtRCbjc9NjWYaISExE\ndBy9c64CuDRMeyHeztpwz9kNxPw4xqKKetYVHeTO86fEuhQRkZgI/CkQXnjfW7a58BQt24hI/xT4\noN+4r5pRg1IYrfPbiEg/FfigL6ioZ4zObSMi/Vjgg76wvI687LRYlyEiEjOBDvrqxhYq61s0oxeR\nfi3QQV9Y7p2WeIzW50WkHwt20Ov88yIiwQ76go4Z/WCt0YtI/xXooC+sqGNw2gDSk3rNRa5ERHpc\noIO+oLxeyzYi0u8FPui1I1ZE+rvABn1zazvFVQ3kaX1eRPq5wAb9nsp62p0OrRQRCWzQF+jQShER\nIMBBry9LiYh4Igp6M8s2s6fMrM7MCsxscRf9rjCzrWZWbWYHzOwhM+t8vdkeUVBeT0piPLkZSbF4\neRGRXiPSGf19QDMwFLgKuN/MpoXp9yZwjnMuExiPd2GTH0Sj0KNVWFFHXnYqZjG/9omISEx1G/Rm\nlgYsAu5yztU651YCzwBXd+7rnCv0LyPYoQ2YGK1ij0ZhhY6hFxGByGb0k4FW59y2kLZ1QLgZPWY2\n38yqgBq8D4h7u+h3g5nlm1l+aWl0rx3unKOwQsfQi4hAZEGfDlR3aqsGMsJ1ds6tdM5lAaOAnwC7\nu+j3gHNutnNudm5ubuQVR+BATRONLe06PbGICJEFfS3QeYdqFt6MvUvOub3ACuCPx1bases4mZm+\nLCUiElnQbwMSzGxSSNtMYGMEz00AJhxLYcejoLwO0KGVIiIQQdA75+qAJ4ElZpZmZvOBi4GHO/c1\ns6vMLM//eQzw/4D/jW7J3SuqbMAMRg5K6emXFhHpdSI9vPIWIAU4ACwDbnbObTSzPDOr7Qh34GTg\nTTOrA94AtgJfiXbR3SmpaiA3PYnE+MB+H0xEJGIRnajdOVcBXBqmvRBvZ23H/f8A/iNq1R2jkuom\nhmclx7oMEZFeIZBT3pKqBoYp6EVEgIAGfXFVI8OztD4vIgIBDPraplZqGls1oxcR8QUu6EuqGgG0\nRi8i4gts0A/LVNCLiEAAg764qgFASzciIr7ABX3HjH6oZvQiIkAQg766key0ASQnxse6FBGRXiF4\nQV/VqPV5EZEQgQt67xh6Bb2ISIfABX1JdaN2xIqIhAhU0De2tFFR16wZvYhIiEAF/f5qHXEjItJZ\noIK++NC3YnWeGxGRDoEK+o4ZvdboRUQ+FFHQm1m2mT1lZnVmVmBmi7vo90UzW2Nm1Wa2x8x+bGYR\nnfM+Gjpm9Ap6EZEPRTqjvw9oBoYCVwH3m9m0MP1Sga8DOcBc4JPAHVGoMyIlVY1kJCeQntRjny0i\nIr1et4loZmnAImC6c64WWGlmzwBXA3eG9nXO3R9yd6+ZPQp8Ior1HlFxVYOOuBER6SSSGf1koNU5\nty2kbR0Qbkbf2ceBjcdS2LEoqWpkmHbEiogcJpKgTweqO7VVAxlHepKZfRmYDdzTxeM3mFm+meWX\nlpZGUmu3iqsaGZaZFJVtiYgERSRBXwtkdmrLAmq6eoKZXQr8EDjfOVcWro9z7gHn3Gzn3Ozc3NxI\n6+1SS1s7pbVNmtGLiHQSSdBvAxLMbFJI20y6WJIxs08D/w1c5Jx7//hLjMyBmiac05WlREQ66zbo\nnXN1wJPAEjNLM7P5wMXAw537mtm5wKPAIufcO9Eu9khKdGiliEhYkR5eeQuQAhwAlgE3O+c2mlme\nmdWaWZ7f7y68ZZ0X/PZaM/tr9Mv+KF0rVkQkvIgOOHfOVQCXhmkvxNtZ23G/xw6l7OzQJQR1nhsR\nkcME5hQI+6sbSUqIIyslMdaliIj0KoEJ+pLqJoZlJWNmsS5FRKRXCUzQ769q1OmJRUTCCEzQl1Tr\nWrEiIuEEIuidc7qEoIhIFwIR9JX1LTS3tmvpRkQkjEAE/aEvSynoRUQ+IhBB/+GVpXRCMxGRzgIR\n9CWHgl4nNBMR6SwYQV/ViBkMydCMXkSks0AE/f7qRganJZEYH4jhiIhEVSCS0Tu0UrN5EZFwghH0\nVfqylIhIVwIR9PurdfoDEZGu9Pmgb2xpo7K+RTN6EZEuRBT0ZpZtZk+ZWZ2ZFZjZ4i76TTezF82s\nzMxcdEsN70B1EwBDdfoDEZGwIp3R3wc0A0OBq4D7zWxamH4twHLguuiU171Dx9BrRi8iEla3V5gy\nszRgETDdOVcLrDSzZ4CrgTtD+zrntgJbzWziiSg2nA+/LKWgFxEJJ5IZ/WSg1Tm3LaRtHRBuRt/j\n9vvnudHOWBGR8CIJ+nSgulNbNZBxPC9sZjeYWb6Z5ZeWlh7zdkqqG0lJjCczOaLL34qI9DuRBH0t\nkNmpLQuoOZ4Xds494Jyb7ZybnZube8zb6TgPvS4hKCISXiRBvw1IMLNJIW0zgY0npqSj411CUN+K\nFRHpSrdB75yrA54ElphZmpnNBy4GHu7c1zzJwAD/frKZndAU1iUERUSOLNLDK28BUoADwDLgZufc\nRjPLM7NaM8vz+40BGvhwtt8AbI1mwaHa2533rVgdcSMi0qWI9mA65yqAS8O0F+LtrO24vxvoscXy\nivpmWtqcZvQiIkfQp0+BoEsIioh0r08HfcclBLV0IyLStT4d9FkpiZw3bSijBukSgiIiXenT3zKa\nPTab2WOzY12GiEiv1qdn9CIi0j0FvYhIwCnoRUQCTkEvIhJwCnoRkYBT0IuIBJyCXkQk4BT0IiIB\nZ865WNeAmZUCBcf49BygLIrl9Gb9Zaz9ZZygsQZRT45zjHOu2ys39YqgPx5mlu+cmx3rOnpCfxlr\nfxknaKxB1BvHqaUbEZGAU9CLiARcEIL+gVgX0IP6y1j7yzhBYw2iXjfOPr9GLyIiRxaEGb2IiByB\ngl5EJOAU9CIiAddng97Mss3sKTOrM7MCM1sc65qiwcySzOx3/phqzGytmZ0f8vgnzWyLmdWb2Stm\nNiaW9UaDmU0ys0YzeySkLYjjvMLMNvu/szvN7Gy/PVBjNbOxZvaCmVWaWYmZ/crMEvzH+uxYzeyr\nZpZvZk1m9mCnx7ocl3nuNrNy/3a3mVlP1t5ngx64D2gGhgJXAfeb2bTYlhQVCUARcA6QBXwHWO7/\nz5MDPAncBWQD+cDjsSo0iu4DVnfcCeI4zeyfgLuBLwEZwMeBXUEcK/BroBQYDszC+12+JQBj3Qf8\nAPh9aGME47oBuBSYCcwALgJu7IF6P+Sc63M3IA0v5CeHtC0FfhTr2k7QeNcDi/xfmDc7vQ8NwJRY\n13gcY7sCWA58D3jEbwviON8ErgvTHsSxbgYuCLn/E+C3QRkrXtg/GOm/of9vf0PI418G3urJmvvq\njH4y0Oqc2xbStg4Iwoz+MGY2FG+8G/HGt67jMedcHbCDPjpuM8sElgD/2umhoI0zHpgN5JrZDjPb\n4y9npBCwsfruBT5vZqlmNhI4H1hBMMcK3Y/rsMeJQVb11aBPB6o7tVXj/UkcGGaWCDwKPOSc24I3\n7qpO3fryuL8P/M45t6dTe9DGORRIBP4ZOBtvOeNUvGW5oI0V4DVgOt449uAtZTxNMMcK3Y+r8+PV\nQHpPrtP31aCvBTI7tWUBNTGo5YQwszjgYbwlqq/6zYEZt5nNAhYCPw/zcGDG6Wvw//tL51yxc64M\n+BlwAQEbq/97uwJvzToN70yOg/D2TwRqrCG6G1fnx7OAWuev4/SEvhr024AEM5sU0jYTb3mjz/M/\n6X+HNxNc5Jxr8R/aiDfOjn5pwAT65rgXAGOBQjMrAe4AFpnZuwRrnDjnKvFmtqH/Y3f8HKix4u2M\nzAN+5Zxrcs6VA3/A+1AL2lg7dDeuwx4nFlkV6x0bx7FD5I/AY3izhvl4fxpNi3VdURrbb4C3gPRO\n7bn+OBcBycCP6eGdOlEcYyowLOR2D/Anf4yBGWfIeJfgHVk0BG+G+zre0lUQx7oL+DbeEWQDgaeA\nZX19rP54koEf4v21ney3HXFcwE14O6hH+rdNwE09Wnus37zjeNOz8db96oBCYHGsa4rSuMbgzfYa\n8f7k67hd5T++ENiCtxzwKjA21jVHadzfwz/qJojjxFuj/zVwECgBfgEkB3Sss/xxVOJdgGM5MLSv\nj9X/HXWdbt/rblyA+eFf4d9+jH+esZ666aRmIiIB11fX6EVEJEIKehGRgFPQi4gEnIJeRCTgFPQi\nIgGnoBcRCTgFvYhIwCnoRUQC7v8DwVg15d1ZCTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15c59e28ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnClassifier.acc_values)\n",
    "plt.title('Accuracy over training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
