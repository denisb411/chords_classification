{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48400\n",
      "39207 39207\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df_labelled_data_path = './data/global_df_data_labelled_20480.pkl'\n",
    "\n",
    "y_chords = pd.read_pickle(df_labelled_data_path)['label']\n",
    "\n",
    "enharmonic_notes = {'C#': 'Db',\n",
    "                   'D#': 'Eb',\n",
    "                   'F#': 'Gb',\n",
    "                   'G#': 'Ab',\n",
    "                   'A#': 'Bb'}\n",
    "\n",
    "chords = ['C', 'C:min', 'C:maj7', 'C:min7',\n",
    "         'C#', 'C#:min', 'C#:maj7', 'C#:min7',\n",
    "        'D', 'D:min', 'D:maj7', 'D:min7',\n",
    "        'D#', 'D#:min', 'D#:maj7', 'D#:min7',\n",
    "        'E', 'E:min', 'E:maj7', 'E:min7',\n",
    "        'F', 'F:min', 'F:maj7', 'F:min7',\n",
    "        'F#', 'F#:min', 'F#:maj7', 'F#:min7',\n",
    "        'G', 'G:min', 'G:maj7', 'G:min7',\n",
    "        'G#', 'G#:min', 'G#:maj7', 'G#:min7',\n",
    "        'A', 'A:min', 'A:maj7', 'A:min7',\n",
    "        'A#', 'A#:min', 'A#:maj7', 'A#:min7',\n",
    "        'B', 'B:min', 'B:maj7', 'B:min7',\n",
    "        'N']\n",
    "\n",
    "# chords = set()\n",
    "# for item in y_chords:\n",
    "#     chords.add(item)\n",
    "\n",
    "chord_to_id = {}\n",
    "id_to_chord = {}\n",
    "for i, chord in enumerate(chords):\n",
    "    chord_to_id[chord] = i\n",
    "    id_to_chord[i] = chord\n",
    "\n",
    "X_notprocessed = pd.read_pickle(df_labelled_data_path)['signal']\n",
    "X = np.zeros((len(X_notprocessed),12,80,1), dtype=np.float)\n",
    "\n",
    "for i, x_notprocessed in enumerate(X_notprocessed):\n",
    "    X[i] = np.atleast_3d(x_notprocessed)\n",
    "    \n",
    "X_filtered = np.zeros((len(X),12,80,1), dtype=np.float)\n",
    "\n",
    "y = []\n",
    "print(len(y_chords))\n",
    "ii = 0\n",
    "for i, chord in enumerate(y_chords):\n",
    "    for key, value in enharmonic_notes.items():\n",
    "        chord = re.sub(value, key, chord)\n",
    "    if chord in chords:\n",
    "        X_filtered[ii] = X[i]\n",
    "        y.append(chord_to_id[chord])\n",
    "        ii+=1\n",
    "        continue\n",
    "X = np.zeros((len(y),12,80,1), dtype=np.float)\n",
    "for i in range(len(y)):\n",
    "    X[i] = X_filtered[i]\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A',\n",
       " 'A/3',\n",
       " 'A/4',\n",
       " 'A/5',\n",
       " 'A/6',\n",
       " 'A/9',\n",
       " 'A/b7',\n",
       " 'A:(1)',\n",
       " 'A:(1,2,4)',\n",
       " 'A:7',\n",
       " 'A:7(#9)',\n",
       " 'A:7(*5,13)',\n",
       " 'A:7(13)',\n",
       " 'A:7/3',\n",
       " 'A:7/5',\n",
       " 'A:7/b7',\n",
       " 'A:9',\n",
       " 'A:9(11)',\n",
       " 'A:aug',\n",
       " 'A:aug/#5',\n",
       " 'A:dim/b3',\n",
       " 'A:dim/b5',\n",
       " 'A:dim7',\n",
       " 'A:maj(9)/3',\n",
       " 'A:maj/9',\n",
       " 'A:maj6',\n",
       " 'A:maj7',\n",
       " 'A:maj7/5',\n",
       " 'A:min',\n",
       " 'A:min(*5)',\n",
       " 'A:min(2)',\n",
       " 'A:min/4',\n",
       " 'A:min/5',\n",
       " 'A:min/6',\n",
       " 'A:min/b3',\n",
       " 'A:min/b7',\n",
       " 'A:min6',\n",
       " 'A:min7',\n",
       " 'A:min7(*5,b6)',\n",
       " 'A:min7(*b3)',\n",
       " 'A:min9',\n",
       " 'A:sus2',\n",
       " 'A:sus4',\n",
       " 'A:sus4(2)',\n",
       " 'A:sus4/5',\n",
       " 'Ab',\n",
       " 'Ab/5',\n",
       " 'Ab/7',\n",
       " 'Ab/b7',\n",
       " 'Ab:7',\n",
       " 'Ab:aug',\n",
       " 'Ab:maj(2)/2',\n",
       " 'Ab:maj(9)',\n",
       " 'Ab:maj6',\n",
       " 'Ab:maj7',\n",
       " 'Ab:maj9',\n",
       " 'Ab:min',\n",
       " 'Ab:min7',\n",
       " 'B',\n",
       " 'B/3',\n",
       " 'B/5',\n",
       " 'B/6',\n",
       " 'B/7',\n",
       " 'B:(b3,5)',\n",
       " 'B:7',\n",
       " 'B:7(#9)',\n",
       " 'B:9',\n",
       " 'B:aug',\n",
       " 'B:aug/3',\n",
       " 'B:dim',\n",
       " 'B:dim7',\n",
       " 'B:dim7/b9',\n",
       " 'B:hdim7/b3',\n",
       " 'B:hdim7/b7',\n",
       " 'B:maj(*3)',\n",
       " 'B:maj6',\n",
       " 'B:maj7',\n",
       " 'B:min',\n",
       " 'B:min/5',\n",
       " 'B:min/6',\n",
       " 'B:min/7',\n",
       " 'B:min/b3',\n",
       " 'B:min/b7',\n",
       " 'B:min7',\n",
       " 'B:sus2',\n",
       " 'B:sus4',\n",
       " 'Bb',\n",
       " 'Bb/3',\n",
       " 'Bb/5',\n",
       " 'Bb/b7',\n",
       " 'Bb:(1)',\n",
       " 'Bb:7',\n",
       " 'Bb:dim7/5',\n",
       " 'Bb:dim7/7',\n",
       " 'Bb:maj',\n",
       " 'Bb:maj(9)/9',\n",
       " 'Bb:maj/9',\n",
       " 'Bb:maj6',\n",
       " 'Bb:maj7',\n",
       " 'Bb:min',\n",
       " 'Bb:min/b3',\n",
       " 'Bb:min7',\n",
       " 'Bb:sus2(b7)',\n",
       " 'Bb:sus4(9)',\n",
       " 'C',\n",
       " 'C#',\n",
       " 'C#:(1,b3)',\n",
       " 'C#:(1,b3)/b3',\n",
       " 'C#:7',\n",
       " 'C#:dim',\n",
       " 'C#:dim/b3',\n",
       " 'C#:hdim7',\n",
       " 'C#:maj7(*b5)',\n",
       " 'C#:maj7/3',\n",
       " 'C#:min',\n",
       " 'C#:min/5',\n",
       " 'C#:min/b7',\n",
       " 'C#:min7',\n",
       " 'C#:sus4',\n",
       " 'C/3',\n",
       " 'C/5',\n",
       " 'C/6',\n",
       " 'C/7',\n",
       " 'C/9',\n",
       " 'C:(1)',\n",
       " 'C:7',\n",
       " 'C:7/3',\n",
       " 'C:7/5',\n",
       " 'C:9',\n",
       " 'C:9(*3)',\n",
       " 'C:aug',\n",
       " 'C:dim7',\n",
       " 'C:dim7/2',\n",
       " 'C:maj(#11)',\n",
       " 'C:maj(#4)/5',\n",
       " 'C:maj(2)',\n",
       " 'C:maj(4)',\n",
       " 'C:maj(9)',\n",
       " 'C:maj6',\n",
       " 'C:maj6/5',\n",
       " 'C:maj7',\n",
       " 'C:maj7(*5)',\n",
       " 'C:maj7/7',\n",
       " 'C:maj9',\n",
       " 'C:min',\n",
       " 'C:min(*b3)',\n",
       " 'C:min6',\n",
       " 'C:sus4',\n",
       " 'D',\n",
       " 'D#:7',\n",
       " 'D#:dim',\n",
       " 'D#:dim/b5',\n",
       " 'D#:dim7',\n",
       " 'D#:hdim7',\n",
       " 'D#:min',\n",
       " 'D/2',\n",
       " 'D/3',\n",
       " 'D/5',\n",
       " 'D/6',\n",
       " 'D/7',\n",
       " 'D/b7',\n",
       " 'D:(1)',\n",
       " 'D:(1,5)',\n",
       " 'D:7',\n",
       " 'D:7(#9)',\n",
       " 'D:7/2',\n",
       " 'D:7/5',\n",
       " 'D:9',\n",
       " 'D:9/5',\n",
       " 'D:dim',\n",
       " 'D:dim7',\n",
       " 'D:maj(*3)',\n",
       " 'D:maj(11)',\n",
       " 'D:maj(2)',\n",
       " 'D:maj(9)',\n",
       " 'D:maj/2',\n",
       " 'D:maj6',\n",
       " 'D:maj6/5',\n",
       " 'D:maj7',\n",
       " 'D:min',\n",
       " 'D:min/2',\n",
       " 'D:min/4',\n",
       " 'D:min/5',\n",
       " 'D:min/6',\n",
       " 'D:min/b3',\n",
       " 'D:min/b7',\n",
       " 'D:min7',\n",
       " 'D:min7(*b3)',\n",
       " 'D:min7(2,*b3,4)',\n",
       " 'D:min7(4)/5',\n",
       " 'D:min7(4)/b7',\n",
       " 'D:min7/4',\n",
       " 'D:min7/b7',\n",
       " 'D:min9',\n",
       " 'D:sus4',\n",
       " 'D:sus4(2)',\n",
       " 'D:sus4(9)',\n",
       " 'D:sus4(b7)',\n",
       " 'D:sus4/5',\n",
       " 'Db',\n",
       " 'Db/5',\n",
       " 'Db:7',\n",
       " 'Db:maj7',\n",
       " 'Db:min',\n",
       " 'E',\n",
       " 'E/#4',\n",
       " 'E/2',\n",
       " 'E/3',\n",
       " 'E/4',\n",
       " 'E/5',\n",
       " 'E/6',\n",
       " 'E/7',\n",
       " 'E/b7',\n",
       " 'E:(1)',\n",
       " 'E:(1,2,5,b6)',\n",
       " 'E:(1,b7)/b7',\n",
       " 'E:7',\n",
       " 'E:7(#9)',\n",
       " 'E:7/3',\n",
       " 'E:7/5',\n",
       " 'E:9',\n",
       " 'E:aug',\n",
       " 'E:aug(9,11)',\n",
       " 'E:dim/b3',\n",
       " 'E:dim7/b3',\n",
       " 'E:maj(b9)',\n",
       " 'E:maj6',\n",
       " 'E:maj7',\n",
       " 'E:min',\n",
       " 'E:min(*3)/5',\n",
       " 'E:min(*5)',\n",
       " 'E:min(*5)/b7',\n",
       " 'E:min(*b3)/5',\n",
       " 'E:min(2)',\n",
       " 'E:min(9)',\n",
       " 'E:min/3',\n",
       " 'E:min/5',\n",
       " 'E:min/b3',\n",
       " 'E:min/b7',\n",
       " 'E:min6',\n",
       " 'E:min7',\n",
       " 'E:min7(*5)/b7',\n",
       " 'E:min7(4)',\n",
       " 'E:min7/b3',\n",
       " 'E:min7/b7',\n",
       " 'E:sus2(b7)',\n",
       " 'E:sus4',\n",
       " 'E:sus4(2)',\n",
       " 'E:sus4(b7)',\n",
       " 'Eb',\n",
       " 'Eb/3',\n",
       " 'Eb/5',\n",
       " 'Eb/6',\n",
       " 'Eb:7',\n",
       " 'Eb:7/b7',\n",
       " 'Eb:9',\n",
       " 'Eb:aug',\n",
       " 'Eb:dim',\n",
       " 'Eb:maj6',\n",
       " 'Eb:min',\n",
       " 'Eb:min7',\n",
       " 'Eb:sus4',\n",
       " 'F',\n",
       " 'F#',\n",
       " 'F#/5',\n",
       " 'F#:(1,4,b5)',\n",
       " 'F#:(1,4,b7)',\n",
       " 'F#:7',\n",
       " 'F#:7(#9)',\n",
       " 'F#:9',\n",
       " 'F#:aug',\n",
       " 'F#:dim',\n",
       " 'F#:dim/b3',\n",
       " 'F#:dim/b7',\n",
       " 'F#:hdim7',\n",
       " 'F#:hdim7/b7',\n",
       " 'F#:maj(9)',\n",
       " 'F#:min',\n",
       " 'F#:min/5',\n",
       " 'F#:min/b3',\n",
       " 'F#:min6',\n",
       " 'F#:min7',\n",
       " 'F#:min9',\n",
       " 'F#:minmaj7',\n",
       " 'F#:sus4',\n",
       " 'F/3',\n",
       " 'F/5',\n",
       " 'F/6',\n",
       " 'F/7',\n",
       " 'F/9',\n",
       " 'F:7',\n",
       " 'F:7(b9)',\n",
       " 'F:7/b7',\n",
       " 'F:9',\n",
       " 'F:aug',\n",
       " 'F:dim',\n",
       " 'F:maj(*3)',\n",
       " 'F:maj(9)',\n",
       " 'F:maj(9)/5',\n",
       " 'F:maj(9)/6',\n",
       " 'F:maj(9)/9',\n",
       " 'F:maj/9',\n",
       " 'F:maj6',\n",
       " 'F:maj6/5',\n",
       " 'F:maj7',\n",
       " 'F:maj9(*7)',\n",
       " 'F:min',\n",
       " 'F:min/5',\n",
       " 'F:min/b7',\n",
       " 'F:min6',\n",
       " 'F:min6/5',\n",
       " 'F:min6/b3',\n",
       " 'F:min7',\n",
       " 'F:min7/b3',\n",
       " 'G',\n",
       " 'G#',\n",
       " 'G#:7',\n",
       " 'G#:aug',\n",
       " 'G#:dim7',\n",
       " 'G#:hdim7',\n",
       " 'G#:min',\n",
       " 'G#:min7',\n",
       " 'G/2',\n",
       " 'G/3',\n",
       " 'G/5',\n",
       " 'G/b7',\n",
       " 'G:(1)',\n",
       " 'G:(1,5)',\n",
       " 'G:(1,b3,4)/b3',\n",
       " 'G:7',\n",
       " 'G:7/3',\n",
       " 'G:9',\n",
       " 'G:9(*3)',\n",
       " 'G:9(*3,11)',\n",
       " 'G:aug',\n",
       " 'G:dim',\n",
       " 'G:dim7',\n",
       " 'G:maj(*1)/5',\n",
       " 'G:maj/9',\n",
       " 'G:maj6',\n",
       " 'G:maj6(9)',\n",
       " 'G:maj6/2',\n",
       " 'G:maj6/3',\n",
       " 'G:maj6/5',\n",
       " 'G:maj7',\n",
       " 'G:maj7/3',\n",
       " 'G:maj7/5',\n",
       " 'G:min',\n",
       " 'G:min(4)',\n",
       " 'G:min(9)/b3',\n",
       " 'G:min/4',\n",
       " 'G:min/5',\n",
       " 'G:min/b3',\n",
       " 'G:min/b7',\n",
       " 'G:min7',\n",
       " 'G:minmaj7',\n",
       " 'G:minmaj7/5',\n",
       " 'G:minmaj7/b3',\n",
       " 'G:sus4',\n",
       " 'G:sus4(2)',\n",
       " 'G:sus4(2)/2',\n",
       " 'G:sus4(b7)',\n",
       " 'G:sus4/5',\n",
       " 'Gb',\n",
       " 'Gb:min',\n",
       " 'N'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chords = set()\n",
    "for item in y_chords:\n",
    "    chords.add(item)\n",
    "chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n",
      "(39207, 12, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "shuffledX, shuffledy = shuffle(X, y)\n",
    "\n",
    "trainRange = int(len(shuffledX) * 0.8)\n",
    "validRange = int(len(shuffledX) * 0.9)\n",
    "testRange = int(len(shuffledX) * 0.1)\n",
    "\n",
    "\n",
    "X_train = np.array(shuffledX[:trainRange], dtype=np.float)\n",
    "y_train = np.array(shuffledy[:trainRange], dtype=np.float)\n",
    "\n",
    "X_valid = np.array(shuffledX[trainRange:validRange], dtype=np.float)\n",
    "y_valid = np.array(shuffledy[trainRange:validRange], dtype=np.float)\n",
    "\n",
    "X_test = np.array(shuffledX[testRange:], dtype=np.float)\n",
    "y_test = np.array(shuffledy[testRange:], dtype=np.float)\n",
    "print(y_test[1])\n",
    "print(shuffledX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "#building a CNN using tensorflow\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "class CNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None, \n",
    "                 height=12, width=80, channels=1, architecture=1, \n",
    "                 conv1={'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':None, 'conv1_activation':tf.nn.elu},\n",
    "                 conv2={'conv2_fmaps':32, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.2, 'conv2_activation':tf.nn.elu}):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.conv1_fmaps = conv1['conv1_fmaps']\n",
    "        self.conv1_ksize = conv1['conv1_ksize']\n",
    "        self.conv1_stride = conv1['conv1_stride']\n",
    "        self.conv1_dropout = conv1['conv1_dropout']\n",
    "        self.conv1_activation = conv1['conv1_activation']\n",
    "        self.conv2_fmaps = conv2['conv2_fmaps']\n",
    "        self.conv2_ksize = conv2['conv2_ksize']\n",
    "        self.conv2_stride = conv2['conv2_stride']\n",
    "        self.conv2_dropout = conv2['conv2_dropout']\n",
    "        self.conv2_activation = conv2['conv2_activation']\n",
    "        self.architecture = architecture\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _cnn(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            if conv1_dropout:\n",
    "                conv1 = tf.layers.dropout(conv1, conv1_dropout, training=self._training)\n",
    "\n",
    "        with tf.name_scope(\"pool1\"):\n",
    "            pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            \n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            conv2_fmaps = self.conv2_fmaps\n",
    "            conv2_ksize = self.conv2_ksize\n",
    "            conv2_stride = self.conv2_stride\n",
    "            conv2_pad = \"SAME\"\n",
    "            conv2_dropout = self.conv2_dropout\n",
    "            conv2_activation = self.conv2_activation\n",
    "            conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                     strides=conv2_stride, padding=conv2_pad,\n",
    "                                     activation=conv2_activation, name=\"conv2\")\n",
    "            if conv2_dropout:\n",
    "                conv2 = tf.layers.dropout(conv2, conv2_dropout, training=self._training)\n",
    "\n",
    "        pool2_fmaps = conv2_fmaps\n",
    "        pool2_flat_shape = int(((self.height/2)/2) * ((self.width/2)/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _cnn2(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"pool1\"):\n",
    "            pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            if conv1_dropout:\n",
    "                pool1 = tf.layers.dropout(pool1, conv1_dropout, training=self._training)\n",
    "            \n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            conv2_fmaps = self.conv2_fmaps\n",
    "            conv2_ksize = self.conv2_ksize\n",
    "            conv2_stride = self.conv2_stride\n",
    "            conv2_pad = \"SAME\"\n",
    "            conv2_dropout = self.conv2_dropout\n",
    "            conv2_activation = self.conv2_activation\n",
    "            conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                     strides=conv2_stride, padding=conv2_pad,\n",
    "                                     activation=conv2_activation, name=\"conv2\")\n",
    "\n",
    "        pool2_fmaps = conv2_fmaps\n",
    "        pool2_flat_shape = int(((self.height/2)/2) * ((self.width/2)/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            if conv2_dropout:\n",
    "                pool2_flat = tf.layers.dropout(pool2_flat, conv2_dropout, training=self._training)\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _cnn3(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            if conv1_dropout:\n",
    "                conv1 = tf.layers.dropout(conv1, conv1_dropout, training=self._training)\n",
    "            \n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            conv2_fmaps = self.conv2_fmaps\n",
    "            conv2_ksize = self.conv2_ksize\n",
    "            conv2_stride = self.conv2_stride\n",
    "            conv2_pad = \"SAME\"\n",
    "            conv2_dropout = self.conv2_dropout\n",
    "            conv2_activation = self.conv2_activation\n",
    "            conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                     strides=conv2_stride, padding=conv2_pad,\n",
    "                                     activation=conv2_activation, name=\"conv2\")\n",
    "\n",
    "        pool2_fmaps = conv2_fmaps\n",
    "        pool2_flat_shape = int((self.height/2) * (self.width/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            if conv2_dropout:\n",
    "                pool2_flat = tf.layers.dropout(pool2_flat, conv2_dropout, training=self._training)\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _cnn4(self, inputs):\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            conv1_fmaps = self.conv1_fmaps #filters\n",
    "            conv1_ksize = self.conv1_ksize\n",
    "            conv1_stride = self.conv1_stride\n",
    "            conv1_activation = self.conv1_activation\n",
    "            conv1_pad = \"SAME\"\n",
    "            conv1_dropout = self.conv1_dropout\n",
    "            conv1 = tf.layers.conv2d(inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                     strides=conv1_stride, padding=conv1_pad,\n",
    "                                     activation=conv1_activation, name=\"conv1\")\n",
    "            if conv1_dropout:\n",
    "                conv1 = tf.layers.dropout(conv1, conv1_dropout, training=self._training)\n",
    "\n",
    "        pool2_fmaps = conv1_fmaps\n",
    "        pool2_flat_shape = int((self.height/2) * (self.width/2) * pool2_fmaps)\n",
    "        with tf.name_scope(\"pool2\"):\n",
    "            pool2 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "            pool2_flat = tf.reshape(pool2, shape=[-1, pool2_flat_shape])\n",
    "            \n",
    "\n",
    "        return pool2_flat\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        X_reshaped = tf.reshape(X, shape=[-1, self.height, self.width, self.channels])\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "        self._training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "        if self.architecture == 1: #conv1 -> Drop -> max_pool -> conv2 -> Drop -> Max_pool\n",
    "            cnn_outputs = self._cnn(X_reshaped)\n",
    "        if self.architecture == 2: #conv1 -> max_pool -> Drop -> conv2 -> max_pool -> Drop\n",
    "            cnn_outputs = self._cnn2(X_reshaped)\n",
    "        if self.architecture == 3: #conv1 -> Drop-> conv2 -> max_pool -> Drop\n",
    "            cnn_outputs = self._cnn3(X_reshaped)\n",
    "        if self.architecture == 4: #conv -> Drop -> max_pool\n",
    "            cnn_outputs = self._cnn4(X_reshaped)\n",
    "\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            dnn_outputs = self._dnn(cnn_outputs)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"output\")\n",
    "            Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(xentropy)\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            training_op = optimizer.minimize(loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        with tf.name_scope(\"init_and_save\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs=300, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = self.height * self.width * self.channels\n",
    "        n_outputs = np.amax(y_train) + 1\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        print(\"***********\")\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "        print(X_valid.shape)\n",
    "        print(y_valid.shape)\n",
    "\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        best_loss_val = np.infty\n",
    "        check_interval = 50\n",
    "        checks_since_last_progress = 0\n",
    "        max_checks_without_progress = 30\n",
    "        best_model_params = None \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            if X_valid is not None and y_valid is not None:\n",
    "                for epoch in range(n_epochs):\n",
    "                    rnd_idx = np.random.permutation(len(X_train))\n",
    "                    idx = 0\n",
    "                    for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
    "                        X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "                        X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                        sess.run(self._training_op, feed_dict={self._X: X_batch_reshaped, self._y: y_batch, self._training: True})\n",
    "                        if idx % check_interval == 0:\n",
    "                            X_valid_reshaped = np.reshape(X_valid,(len(X_valid), -1))\n",
    "                            loss_val = self._loss.eval(feed_dict={self._X: X_valid_reshaped,\n",
    "                                                            self._y: y_valid})\n",
    "                            if loss_val < best_loss_val:\n",
    "                                best_loss_val = loss_val\n",
    "                                checks_since_last_progress = 0\n",
    "                                best_model_params = self._get_model_params()\n",
    "                            else:\n",
    "                                checks_since_last_progress += 1\n",
    "                        idx += 1\n",
    "                    X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                    acc_train = self._accuracy.eval(feed_dict={self._X: X_batch_reshaped, self._y: y_batch})\n",
    "                    X_valid_reshaped = np.reshape(X_valid,(len(X_valid), -1))\n",
    "                    acc_val = self._accuracy.eval(feed_dict={self._X: X_valid_reshaped,\n",
    "                                                       self._y: y_valid})\n",
    "                    print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                              epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
    "                    if checks_since_last_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "\n",
    "                if best_model_params:\n",
    "                    self._restore_model_params(best_model_params)\n",
    "            else: \n",
    "                for epoch in range(n_epochs):\n",
    "                    rnd_idx = np.random.permutation(len(X_train))\n",
    "                    idx = 0\n",
    "                    for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
    "                        X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "                        X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                        sess.run(self._training_op, feed_dict={self._X: X_batch_reshaped, self._y: y_batch, self._training: True})\n",
    "\n",
    "                    X_batch_reshaped = np.reshape(X_batch,(len(X_batch), -1))\n",
    "                    acc_train = self._accuracy.eval(feed_dict={self._X: X_batch_reshaped, self._y: y_batch})\n",
    "                    print(\"Epoch {}, train accuracy: {:.4f}%\".format(\n",
    "                              epoch, acc_train * 100))\n",
    "\n",
    "            return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            X_reshaped = np.reshape(X,(len(X), -1))\n",
    "            return np.argmax(self._Y_proba.eval(feed_dict={self._X: X_reshaped}))\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)\n",
    "\n",
    "    def restore(self, path, n_inputs=960, n_outputs=48):\n",
    "        self.close_session()\n",
    "\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        \n",
    "        self._saver.restore(self._session, path)\n",
    "\n",
    "    def accuracy_score(self, X_test, y_test):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            X_test_reshaped = np.reshape(X_test,(len(X_test), -1))\n",
    "            acc_test = self._accuracy.eval(feed_dict={self._X: X_test_reshaped, self._y: y_test})\n",
    "            print(\"Final accuracy on test set:\", acc_test)\n",
    "\n",
    "            return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********\n",
      "(31365, 12, 80, 1)\n",
      "(31365,)\n",
      "(3921, 12, 80, 1)\n",
      "(3921,)\n",
      "Epoch 0, train accuracy: 15.0000%, valid. accuracy: 21.7292%, valid. best loss: 2.662328\n",
      "Epoch 1, train accuracy: 25.0000%, valid. accuracy: 21.2956%, valid. best loss: 2.653306\n",
      "Epoch 2, train accuracy: 30.0000%, valid. accuracy: 20.6070%, valid. best loss: 2.635620\n",
      "Epoch 3, train accuracy: 20.0000%, valid. accuracy: 22.1882%, valid. best loss: 2.619252\n",
      "Epoch 4, train accuracy: 10.0000%, valid. accuracy: 22.3412%, valid. best loss: 2.619252\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNClassifier(activation=<function elu at 0x00000180F5ACB9D8>, architecture=1,\n",
       "       batch_norm_momentum=None, batch_size=20, channels=1, conv1=None,\n",
       "       conv2=None, dropout_rate=None, height=12,\n",
       "       initializer=<function variance_scaling_initializer.<locals>._initializer at 0x00000180FAA8A840>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=None, width=80)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnClassifier = CNNClassifier(architecture=1, height=12, width=80)\n",
    "cnnClassifier.fit(X_train, y_train, n_epochs=300, X_valid=X_valid, y_valid=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test set: 0.24861847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24861847"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnClassifier.accuracy_score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random search implementation of CNN\n",
    "class RandomSearchCNN(object):\n",
    "    def __init__(self, params, k_fold=5, num_random_combinations=100):\n",
    "        \"\"\"GridSearch on CNN with cross_validation with k_fold = 5\"\"\"\n",
    "        self.best_params = None\n",
    "        self.k_fold = k_fold\n",
    "        self.params = {\n",
    "                        'n_hidden_layers':params['n_hidden_layers'],\n",
    "                        'n_neurons':params['n_neurons'],\n",
    "                        'optimizer_class':params['optimizer_class'],\n",
    "                        'learning_rate':params['learning_rate'],\n",
    "                        'dropout_rate':params['dropout_rate'],\n",
    "                        'batch_size':params['batch_size'],\n",
    "                        'activation':params['activation'],\n",
    "                        'conv1':params['conv1'],\n",
    "                        'conv2':params['conv2'],\n",
    "                        'architecture':params['architecture']\n",
    "        }\n",
    "\n",
    "        max_indexes = [len(v) for k, v in self.params.items()]\n",
    "        num_random_combinations = min(num_random_combinations, np.prod(max_indexes))\n",
    "\n",
    "        # generate unique combinations\n",
    "        combinations = set()\n",
    "        while len(combinations) < num_random_combinations:\n",
    "            combinations.add(tuple(\n",
    "                random.randint(0, max_index - 1)\n",
    "                for max_index in max_indexes))\n",
    "        # make sure their order is shuffled\n",
    "        # (`set` seems to sort its content)\n",
    "        combinations = list(combinations)\n",
    "        random.shuffle(combinations)\n",
    "\n",
    "        self.combinations = combinations\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test=None, y_test=None, X_valid=None, y_valid=None, log_name='randomSearchCNN_results.txt'):\n",
    "\n",
    "        scores = []\n",
    "        print(\"testing\", len(self.combinations), \"combinations using kfold =\", self.k_fold, \".\")\n",
    "        for combination in self.combinations:\n",
    "            accuracy_rate = 0\n",
    "            folds = self.k_fold\n",
    "            if folds <= 1:\n",
    "                if X_test == None or y_test == None:\n",
    "                    raise ValueError(\"Pass the test set when using kfold = 1!\")\n",
    "\n",
    "                print(\"Trining CNN with parameters: \" + \n",
    "                        \"n_hidden_layers: %d, \" % (self.params['n_hidden_layers'][combination[0]]) +\n",
    "                        \"n_neurons: %d, \" % (self.params['n_neurons'][combination[1]]) +\n",
    "                        \"optimizer_class: %s, \" % (self.params['optimizer_class'][combination[2]]) +\n",
    "                        \"learning_rate: %d, \" % (self.params['learning_rate'][combination[3]]) +\n",
    "                        \"dropout_rate: %d, \" % (self.params['dropout_rate'][combination[4]]) +\n",
    "                        \"batch_size: %d, \" % (self.params['batch_size'][combination[5]]) +\n",
    "                        \"activation: %s, \" % (self.params['activation'][combination[6]]) +\n",
    "                        \"conv1: %s, \" % (self.params['conv1'][combination[7]]) +\n",
    "                        \"conv2: %s .\" % (self.params['conv2'][combination[8]]) +\n",
    "                        \"architecture: %d, \" % (self.params['architecture'][combination[9]])\n",
    "                    )\n",
    "\n",
    "                n_hidden_layers = self.params['n_hidden_layers'][combination[0]]\n",
    "                n_neurons = self.params['n_neurons'][combination[1]]\n",
    "                optimizer_class = self.params['optimizer_class'][combination[2]]\n",
    "                learning_rate = self.params['learning_rate'][combination[3]]\n",
    "                dropout_rate = self.params['dropout_rate'][combination[4]]\n",
    "                batch_size = self.params['batch_size'][combination[5]]\n",
    "                activation = self.params['activation'][combination[6]]\n",
    "                conv1 = self.params['conv1'][combination[7]]\n",
    "                conv2 = self.params['conv2'][combination[8]]\n",
    "                architecture = self.params['architecture'][combination[9]]\n",
    "\n",
    "                cnn = CNNClassifier(n_hidden_layers=n_hidden_layers, n_neurons=n_neurons, \n",
    "                    optimizer_class=optimizer_class,\n",
    "                    learning_rate=learning_rate, batch_size=batch_size, \n",
    "                    dropout_rate=dropout_rate, activation=activation, \n",
    "                    conv1=conv1, conv2=conv2, architecture=architecture)\n",
    "\n",
    "                cnn.fit(X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid)\n",
    "                accuracy_rate += cnn.accuracy_score(X_test, y_test)\n",
    "                del cnn\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    X = np.vstack([X_train, X_test])\n",
    "                    y = np.append(y_train, y_test)\n",
    "                    print('here')\n",
    "                except:\n",
    "                    X = X_train\n",
    "                    y = y_train\n",
    "                k_fold_samples = int(len(X) / folds)\n",
    "                print(\"Trining CNN with parameters: \" + \n",
    "                            \"n_hidden_layers: %d, \" % (self.params['n_hidden_layers'][combination[0]]) +\n",
    "                            \"n_neurons: %d, \" % (self.params['n_neurons'][combination[1]]) +\n",
    "                            \"optimizer_class: %s, \" % (self.params['optimizer_class'][combination[2]]) +\n",
    "                            \"learning_rate: %d, \" % (self.params['learning_rate'][combination[3]]) +\n",
    "                            \"dropout_rate: %d, \" % (self.params['dropout_rate'][combination[4]]) +\n",
    "                            \"batch_size: %d, \" % (self.params['batch_size'][combination[5]]) +\n",
    "                            \"activation: %s, \" % (self.params['activation'][combination[6]]) +\n",
    "                            \"conv1: %s, \" % (self.params['conv1'][combination[7]]) +\n",
    "                            \"conv2: %s .\" % (self.params['conv2'][combination[8]]) +\n",
    "                            \"architecture: %d, \" % (self.params['architecture'][combination[9]])\n",
    "                        )\n",
    "                for k in range(folds):\n",
    "                    n_hidden_layers = self.params['n_hidden_layers'][combination[0]]\n",
    "                    n_neurons = self.params['n_neurons'][combination[1]]\n",
    "                    optimizer_class = self.params['optimizer_class'][combination[2]]\n",
    "                    learning_rate = self.params['learning_rate'][combination[3]]\n",
    "                    dropout_rate = self.params['dropout_rate'][combination[4]]\n",
    "                    batch_size = self.params['batch_size'][combination[5]]\n",
    "                    activation = self.params['activation'][combination[6]]\n",
    "                    conv1 = self.params['conv1'][combination[7]]\n",
    "                    conv2 = self.params['conv2'][combination[8]]\n",
    "                    architecture = self.params['architecture'][combination[9]]\n",
    "\n",
    "                    print(\"Training and testing fold %i\" % k)\n",
    "                    range1 = k_fold_samples * ((folds - 1) - k)\n",
    "                    range2 = k_fold_samples * (folds - k)\n",
    "\n",
    "                    print(\"ranges:\")\n",
    "                    print(range1, range2)\n",
    "\n",
    "                    X_train_step = X[:range1]\n",
    "                    X_train_step = np.vstack([X_train, X[range2:]])\n",
    "                    y_train_step = y[:range1]\n",
    "                    y_train_step = np.append(y_train, y[range2:])\n",
    "\n",
    "                    X_test_step = X[range1:range2]\n",
    "                    y_test_step = y[range1:range2]\n",
    "\n",
    "                    print(\"shapes:\")\n",
    "                    print(X_train_step.shape, X_test_step.shape)\n",
    "                    print(y_train_step.shape, y_test_step.shape)\n",
    "\n",
    "                    cnn = CNNClassifier(n_hidden_layers=n_hidden_layers, n_neurons=n_neurons, \n",
    "                        optimizer_class=optimizer_class,\n",
    "                        learning_rate=learning_rate, batch_size=batch_size, \n",
    "                        dropout_rate=dropout_rate, activation=activation, \n",
    "                        conv1=conv1, conv2=conv2, architecture=architecture)\n",
    "\n",
    "                    cnn.fit(X_train=X_train_step, y_train=y_train_step, X_valid=X_valid, y_valid=y_valid)\n",
    "                    accuracy_rate += cnn.accuracy_score(X_test_step, y_test_step)\n",
    "\n",
    "            final_accuracy_rate = accuracy_rate / folds\n",
    "            score = {'n_hidden_layers': n_hidden_layers,\n",
    "                            'n_neurons' : n_neurons,\n",
    "                            'optimizer_class' : optimizer_class,\n",
    "                            'learning_rate' : learning_rate,\n",
    "                            'batch_size' : batch_size,\n",
    "                            'activation' : activation,\n",
    "                            'dropout_rate' : dropout_rate,\n",
    "                            'conv1' : conv1,\n",
    "                            'conv2' : conv2,\n",
    "                            'architecture' : architecture,\n",
    "                            'accuracy_rate' : final_accuracy_rate,\n",
    "                        }\n",
    "            scores.append(score)\n",
    "            print(\"******************************************\")\n",
    "            print(score)\n",
    "            print(\"******************************************\")\n",
    "\n",
    "            this_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            search_dir = this_dir + '/search_results'\n",
    "            if os.path.isdir(search_dir) == False:\n",
    "                os.makedirs(search_dir)\n",
    "\n",
    "            search_file = search_dir + '/' + log_name\n",
    "            with open(search_file,\"a\") as f:\n",
    "                f.write(str(score) + \"\\n\")\n",
    "\n",
    "\n",
    "        best_score = 0\n",
    "        for score in scores:\n",
    "            accuracy = score['accuracy_rate']\n",
    "            if accuracy > best_score:\n",
    "                best_score = accuracy\n",
    "                self.best_params = score\n",
    "\n",
    "        print(\"Best parameters:\")\n",
    "        print(self.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "\t\tdef parametrized_leaky_relu(z, name=None):\n",
    "\t\t\treturn tf.maximum(alpha * z, z, name=name)\n",
    "\t\treturn parametrized_leaky_relu\n",
    "    \n",
    "\"\"\"\n",
    "Best parameters (kfold = 5):\n",
    "\n",
    "{'n_hidden_layers': 2, 'n_neurons': 500, 'optimizer_class': <class 'tensorflow.python.training.adam.AdamOptimizer'>, \n",
    "'learning_rate': 0.05, 'batch_size': 400, \n",
    "'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000203351C67B8>, \n",
    "'dropout_rate': 0.1, 'conv1': {'conv1_fmaps': 16, 'conv1_ksize': 5, 'conv1_stride': 1, \n",
    "'conv1_dropout': 0.3, 'conv1_activation': <function relu at 0x00000203335AC620>}, 'conv2': {'conv2_fmaps': 16, \n",
    "'conv2_ksize': 5, 'conv2_stride': 1, 'conv2_dropout': 0.2, 'conv2_activation': <function relu at 0x00000203335AC620>}, \n",
    "'architecture': 4, 'accuracy_rate': 0.9958441495895386}\n",
    "\n",
    "\n",
    "Best parameters (kfold = 1 and cutting freq = 1000):\n",
    "\n",
    "Best parameters:\n",
    "{'n_hidden_layers': 8, 'n_neurons': 500, 'optimizer_class': <class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "'learning_rate': 0.025, 'batch_size': 600,\n",
    "'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x000001FDB8747950>,\n",
    "'dropout_rate': 1e-06, \n",
    "'conv1': {'conv1_fmaps': 16, 'conv1_ksize': 5, 'conv1_stride': 1, 'conv1_dropout': 0.3, 'conv1_activation': <function relu at 0x000001FDA77410D0>}, \n",
    "'conv2': {'conv2_fmaps': 16, 'conv2_ksize': 5, 'conv2_stride': 1, 'conv2_dropout': 0.3, 'conv2_activation': <function relu at 0x000001FDA77410D0>}, 'architecture': 1, 'accuracy_rate': 0.98441553115844727}\n",
    "\n",
    "Best parameters (kfold = 5 and cutting freq = 1000):\n",
    "\n",
    "{'n_hidden_layers': 2, 'n_neurons': 150, 'optimizer_class': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.025, 'batch_size': 300,\n",
    "'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000213B22AFD90>,\n",
    "'dropout_rate': 0.2, \n",
    "'conv1': {'conv1_fmaps': 16, 'conv1_ksize': 5, 'conv1_stride': 1, 'conv1_dropout': 0.2, 'conv1_activation': <function relu at 0x00000213B6A4FA60>},\n",
    "'conv2': {'conv2_fmaps': 16, 'conv2_ksize': 5, 'conv2_stride': 1, 'conv2_dropout': 0.3, 'conv2_activation': <function relu at 0x00000213B6A4FA60>}, 'architecture': 4, 'accuracy_rate': 0.97958478927612302}\n",
    "\n",
    "Best parameters (kfold = 5 and cutting freq = 500):\n",
    "{'n_hidden_layers': 1, 'n_neurons': 300, 'optimizer_class': <class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
    "'learning_rate': 0.05, 'batch_size': 600, \n",
    "'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x000001D24838D378>, 'dropout_rate': 1e-06, \n",
    "'conv1': {'conv1_fmaps': 16, 'conv1_ksize': 5, 'conv1_stride': 1, 'conv1_dropout': 0.3, 'conv1_activation': <function relu at 0x000001D24678D950>}, \n",
    "'conv2': {'conv2_fmaps': 16, 'conv2_ksize': 5, 'conv2_stride': 1, 'conv2_dropout': 0.2, 'conv2_activation': <function relu at 0x000001D24678D950>},\n",
    "'architecture': 1, 'accuracy_rate': 0.9822956919670105}\n",
    "\n",
    "Best parameters (kfold = 1 and cutting freq = 500):\n",
    "\n",
    "Best parameters:\n",
    "{'n_hidden_layers': 1, 'n_neurons': 400, 'optimizer_class': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.05,\n",
    "'batch_size': 200, 'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x00000243DAF0A598>,\n",
    "'dropout_rate': 0.1,\n",
    "'conv1': {'conv1_fmaps': 16, 'conv1_ksize': 5, 'conv1_stride': 1, 'conv1_dropout': 0.3, 'conv1_activation': <function relu at 0x00000243D798D9D8>},\n",
    "'conv2': {'conv2_fmaps': 16, 'conv2_ksize': 5, 'conv2_stride': 1, 'conv2_dropout': 0.3, 'conv2_activation': <function relu at 0x00000243D798D9D8>},\n",
    "'architecture': 1, 'accuracy_rate': 0.99299889802932739}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "params = {'n_hidden_layers': [1,2,3,4,5,6,7,8,9,10],\n",
    "\t'n_neurons' : [50, 150, 250, 300, 350, 400, 450, 500],\n",
    "\t'optimizer_class' : [tf.train.AdamOptimizer, tf.train.AdagradOptimizer],\n",
    "\t'learning_rate' : [0.01, 0.05, 0.1, 0.025],\n",
    "\t'batch_size' : [200, 250, 300, 400, 600],\n",
    "\t'activation' : [leaky_relu(), tf.nn.elu],\n",
    "\t'dropout_rate' : [0.000001, 0.1, 0.2, 0.3, 0.4],\n",
    "\t'conv1' : [{'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':0.1, 'conv1_activation':tf.nn.elu},\n",
    "               {'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':0.2, 'conv1_activation':tf.nn.relu},\n",
    "               {'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':0.3, 'conv1_activation':tf.nn.relu},\n",
    "               {'conv1_fmaps':16, 'conv1_ksize':5, 'conv1_stride':1, 'conv1_dropout':0.1, 'conv1_activation':tf.nn.elu}],\n",
    "\t'conv2' : [{'conv2_fmaps':16, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.1, 'conv2_activation':tf.nn.elu},\n",
    "               {'conv2_fmaps':16, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.2, 'conv2_activation':tf.nn.relu},\n",
    "               {'conv2_fmaps':16, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.3, 'conv2_activation':tf.nn.relu},\n",
    "               {'conv2_fmaps':16, 'conv2_ksize':5, 'conv2_stride':1, 'conv2_dropout':0.1, 'conv2_activation':tf.nn.elu}],\n",
    "    'architecture' : [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "rnd_search = RandomSearchCNN(params, k_fold=5, num_random_combinations=300)\n",
    "rnd_search.fit(X_train, y_train, X_train, y_train, X_valid=X_valid, y_valid=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
